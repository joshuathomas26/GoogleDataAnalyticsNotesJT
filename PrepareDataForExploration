Week 1

All that data is usually generated as a result of our activity in the world. These days, we spend a lot of time online. With social media and mobile devices, millions and millions of people are adding to the huge amount of data out there, each and every day.
This data generation and collection comes with a few more things to think about. It needs to be done with consideration to ethics so that we maintain people's rights and privacy

There's all kinds of data being generated all the time, and there's lots of different ways to collect it. Even something as simple as an interview can help someone collect data
Scientists also generate data. They use a lot of observations in their work.

Have you ever wondered why some online ads seem to make really accurate suggestions or how some websites remember your preferences? This is done using cookies, which are small files stored on computers that contain information about users. Cookies can help inform advertisers about your personal interests and habits based on your online surfing, without personally identifying you.
As a real world analyst, you'll have all kinds of data right at your fingertips and lots of it too. Knowing how it's been generated can help add context to the data, and knowing how to collect it can make the data analysis process more efficient


Usually, you'll have a head start in figuring out the right data for the job, because the data you need will be given to you, or your business task or problem will narrow down your choices
First, you need to know how the data will be collected

That brings us to data sources.

First-party data. This is data collected by an individual or group using their own resources. Collecting first-party data is typically the preferred method because you know exactly where it came from.
You might also have second-party data, which is data collected by a group directly from its audience and then sold
The same can't always be said about third-party data or data collected from outside sources who did not collect it directly. This data might have come from a number of different sources before you investigated it. It might not be as reliable, but that doesn't mean it can't be useful.

Now let's talk about how much data to collect.
In data analytics, a population refers to all possible data values in a certain data set

But collecting data from the entire population can be pretty challenging. That's why a sample can be useful. A sample is a part of a population that is representative of the population. You might collect a data sample about one spot in the city and analyze the traffic there, or you might pull a random sample from all existing data in the population. How you choose your sample will depend on your project.

In our example, if you needed an answer immediately, you'd have to use historical data, which is data that already exists. But let's say you needed to track traffic patterns over a long period of time. That might affect the other decisions you make during data collection. Now you know more about the different data collection considerations you'll use as a data analyst



This is qualitative data because it can't be counted, measured, or easily expressed using numbers. Qualitative data is usually listed as a name, category, or description. In our spreadsheet, the movie titles and cast members are qualitative data.
Next up is quantitative data, which can be measured or counted and then expressed as a number. This is data with a certain quantity, amount, or range. In our spreadsheet here, the last two columns show the movies's budget and box office revenue.

Let's check out discrete data first. This is data that's counted and has a limited number of values.
Continuous data can be measured using a timer, and its value can be shown as a decimal with several places
There's also nominal and ordinal data.

Nominal data is a type of qualitative data that's categorized without a set order.
In other words, this data doesn't have a sequence

Ordinal data, on the other hand, is a type of qualitative data with a set order or scale. If you asked a group of people to rank a movie from 1 to 5, some might rank it as a 2, others a 4, and so on

Now let's talk about internal data, which is data that lives within a company's own systems
External data is, you guessed it, data that lives and is generated outside of an organization.

Structured data is data that's organized in a certain format, such as rows and columns.
Unstructured data might have internal structure, but the data doesn't fit neatly in rows and columns like structured data.

Most of the data being generated right now is actually unstructured. Audio files, video files, emails, photos, and social media are all examples of unstructured data. These can be harder to analyze in their unstructured format
But here's the good news, you'll be working with structured data most of the time. For example, if you need to analyze data about the unstructured data in emails, photos, and social media sites, it'll most likely be structured for analysis before you even get to it.

Structured data works nicely within a data model, which is a model that is used for organizing data elements and how they relate to one another.
What are data elements? They're pieces of information, such as people's names, account numbers, and addresses

Data models help to keep data consistent and provide a map of how data is organized. This makes it easier for analysts and other stakeholders to make sense of their data and use it for business purposes

In addition to working well within data models, structured data is also useful for databases. This makes it easy for analysts to enter, query, and analyze the data whenever they need to.

Types of data modeling:

Conceptual data modeling gives a high-level view of the data structure, such as how data interacts across an organization. For example, a conceptual data model may be used to define the business requirements for a new database. A conceptual data model doesn't contain technical details.
Logical data modeling focuses on the technical details of a database such as relationships, attributes, and entities. For example, a logical data model defines how individual records are uniquely identified in a database. But it doesn't spell out actual names of database tables. That's the job of a physical data model.
Physical data modeling depicts how a database operates. A physical data model defines all entities and attributes used; for example, it includes table names, column names, and data types for the database.

There are a lot of approaches when it comes to developing data models, but two common methods are the Entity Relationship Diagram (ERD) and the Unified Modeling Language (UML) diagram. ERDs are a visual way to understand the relationship between entities in the data model. UML diagrams are very detailed diagrams that describe the structure of a system by showing the system's entities, attributes, operations, and their relationships.


A data type is a specific kind of data attribute that tells what kind of value the data is

Data types can be different depending on the query language you're using. For example, SQL allows for different data types depending on which database you're using.
Let's focus on the data types that you'll use in spreadsheets

Now a data type in a spreadsheet can be one of three things: a number, a text or string, or a Boolean.

What do a music playlist, a calendar agenda, and an email inbox have in common? I'll give you a hint. It's not a weekly jam session. The answer is they're all arranged in tables.

A data table, or tabular data, has a very simple structure. It's arranged in rows and columns.
You can call the rows "records" and the columns "fields." They basically mean the same thing, but records and fields can be used for any kind of data table, while rows and columns are usually reserved for spreadsheets.

When talking about structured databases, people in data analytics usually go with "records" and "fields." Sometimes a field can also refer to a single piece of data, like the value in a cell.


But the words "wide" and "long" can be used to describe data, too. So I am here to help you understand wide data and long data.

With wide data, every data subject has a single row with multiple columns to hold the values of various attributes of the subject.
Wide data lets you easily identify and quickly compare different columns

Long data is data in which each row is one time point per subject, so each subject will have data in multiple rows.
With this long data format, we can store and analyze all of this data using fewer columns. Plus, if we added a new variable, like the average age of a population, we'd only need one more column. If we'd use a wide data format instead, we would have needed 10 more columns, one for each year. The long data format keeps everything nice and compact.


That reminds me: earlier we define data as a collection of facts. As you've discovered over the last few videos, that collection of facts can take on lots of different formats, structures, types, and more.

Data transformation is the process of changing the data’s format, structure, or values

Data transformation usually involves:

Adding, copying, or replicating data
Deleting fields or records
Standardizing the names of variables
Renaming, moving, or combining columns in a database
Joining one set of data with another
Saving a file in a different format. For example, saving a spreadsheet as a comma separated values (CSV) file.

Goals for data transformation might be:

Data organization: better organized data is easier to use
Data compatibility: different applications or systems can then use the same data
Data migration: data with matching formats can be moved from one system to another
Data merging: data with the same organization can be merged together
Data enhancement: data can be displayed with more detailed fields
Data comparison: apples-to-apples comparisons of the data can then be made

Long data is data where each row contains a single data point for a particular item.
Wide data is data where each row contains multiple data points for the particular items identified in the columns.

Wide data is preferred when:

Creating tables and charts with a few variables about each subject
Comparing straightforward line graphs


Long data is preferred when:

Storing a lot of variables about each subject. For example, 60 years worth of interest rates for each bank
Performing advanced statistical analysis or graphing


Exploring Kaggle:

Step 1: Go to the Code home page
First, go to the Navigation bar on the left side of your screen. Then, click on the Code icon. This takes you to the Code home page.


Create a Kaggle account

To get started, follow these steps to create a Kaggle account.

Note: Kaggle frequently updates its user interface. The latest changes may not be reflected in the screenshots, but the principles in this activity remain the same. Adapting to changes in software updates is an essential skill for data analysts, and we encourage you to practice troubleshooting. You can also reach out to your community of learners on the discussion forum for help.

1. Go to kaggle.com

2. Click on the Register button at the top-right of the Kaggle homepage. You can register with your Google credentials or your personal email address.

screenshot of kaggle homepage. The register button is highlighted
3. Once you’re registered and logged in to Kaggle, click on the Account icon at the top-right of your screen. In the menu that opens, click the Your Profile button.


4. On your profile page, click on the Edit Profile button. Enter any information you’d like to share with the Kaggle community. Your profile will be public, so only enter the information you’re comfortable sharing.


5. If you want some inspiration, check out the profile of Kaggle’s Community Advocate, Jesse Mostipak!

Explore Kaggle notebooks

Now that you’ve created an account and set up your profile, you can check out some notebooks on Kaggle. Kagglers use notebooks to share datasets and data analyses.

Step 1: Go to the Code home page
First, go to the Navigation bar on the left side of your screen. Then, click on the Code icon. This takes you to the Code home page.


Step 2: Review Kaggler contributions
On the Code home page, you’ll notice links to notebooks created by other Kagglers.

To begin, feel free to scroll through the list and click on notebooks that interest you. As you explore, you may come across unfamiliar terms and new information: That’s fine! Kagglers come from diverse backgrounds and focus on different areas of data analysis, data science, machine learning, and deep learning.

Step 3: Narrow your search
Once you’re familiar with the Code home page, you can narrow your search results by typing a word in the search bar or by using the filter feature.

For example, type Beginner in the search bar to show notebooks tagged as beginner-friendly. Or, click on the Filter icon, the triangle shape on the right side of the search bar. You can filter results by tags, programming language, output, and other options. Filter to Datasets to show notebooks that use one of the tens of thousands of public datasets available on Kaggle.

Editing a Kaggle Notebook:

1. Click on the link to open up the notebook. It contains the dataset you’ll work with later on.
2. Click on the Copy and Edit button at the top-right to make a copy of the notebook in your account. Now, the notebook appears in Edit mode. Edit mode lets you make changes to the notebook if you want.

Working with Kaggle Datasets:

1. Click on this title. Two .csv files appear: penguins_lter.csv and penguins_size.csv. Click on one of them. At the bottom of the notebook, you’ll now find an interactive data table with all the information from the dataset.
2. Click on the other .csv file. This opens a second tab with the second dataset.
3. Take a moment to check out each dataset.
4. Sort the data in each column by clicking on the horizontal bars to the right of each column name.
5. Click on the button that says 10 of 17 columns to change the columns that are visible in the table.

In the dropdown menu, there’s a checkmark next to the name of each column that appears in the table. Checking or unchecking one of these boxes will change what data is presented.


Week 2

Bias has evolved to become a preference in favor of or against a person, group of people, or thing. It can be conscious or subconscious. The good news is once we know and accept that we have bias, we can start to recognize our own patterns of thinking and learn how to manage it.
Data bias is a type of error that systematically skews results in a certain direction.

Bias can also happen if a sample group lacks inclusivity.
When we're rushed, we make more mistakes, which can affect the quality of our data and create biased outcomes.

When data is biased, it can systematically skew results in a certain direction, making them unreliable.

Sampling bias is when a sample isn't representative of the population as a whole. You can avoid this by making sure the sample is chosen at random, so that all parts of the population have an equal chance of being included.
If you don't use random sampling during data collection, you end up favoring one outcome.

Unbiased sampling results in a sample that's representative of the population being measured. Another great way to discover if you're working with unbiased data is to bring the results to life with visualizations.

As a quick refresher, sampling bias, is when a sample isn't representative of the population as a whole
In this video, we'll explore three more types of data bias, observer bias, interpretation bias, and confirmation bias, and we'll learn how to avoid them

Observer bias, which is sometimes referred to as experimenter bias or research bias. Basically, it's the tendency for different people to observe things differently.
Interpretation bias. The tendency to always interpret ambiguous situations in a positive, or negative way.
Confirmation bias, is the tendency to search for, or interpret information in a way that confirms preexisting beliefs. Someone might be so eager to confirm a gut feeling, that they only notice things that support it, ignoring all other signals.

Let's learn how we can go about finding and identifying good data sources. First things first, we need to learn how to identify them.
A process I like to call ROCCC, R-O-C-C-C.
R for reliable.
O is for original.
Time for the first C. C is for comprehensive
The next C is for current.
The last C is for cited

Data ethics refers to well- founded standards of right and wrong that dictate how data is collected, shared, and used.
Data ethics tries to get to the root of the accountability companies have in protecting and responsibly using the data they collect. There are lots of different aspects of data ethics but we'll cover six: ownership, transaction transparency, consent, currency, privacy, and openness

First up is ownership. This answers the question who owns data? It isn't the organization that invested time and money collecting, storing, processing, and analyzing it. It's individuals who own the raw data they provide, and they have primary control over its usage, how it's processed and how it's shared.
Next, we have transaction transparency, which is the idea that all data processing activities and algorithms should be completely explainable and understood by the individual who provides their data.
Now let's talk about another aspect of data ethics, consent. This is an individual's right to know explicit details about how and why their data will be used before agreeing to provide it. They should know answers to questions like why is the data being collected? How will it be used? How long will it be stored? The best way to give consent is probably a conversation between the person providing the data and the person requesting it.
Next, there's currency. Individuals should be aware of financial transactions resulting from the use of their personal data and the scale of these transactions.
Data privacy is big in today's culture, so let's explore it fully. When talking about data, privacy means preserving a data subject's information and activity any time a data transaction occurs. This is sometimes called information privacy or data protection. It's all about access, use, and collection of data. It also covers a person's legal right to their data.
Data anonymization is the process of protecting people's private or sensitive data by eliminating PII

Personally identifiable information, or PII, is information that can be used by itself or with other data to track down a person's identity.

de-identification, which is a process used to wipe data clean of all personally identifying information.

Data anonymization is used in just about every industry. That is why it is so important for data analysts to understand the basics. Here is a list of data that is often anonymized:

Telephone numbers
Names
License plates and license numbers
Social security numbers
IP addresses
Medical records
Email addresses
Photographs
Account numbers

In data analytics, open data is part of data ethics, which has to do with using data ethically. Openness refers to free access, usage, and sharing of data.
But for data to be considered open, it has to:

Be available and accessible to the public as a complete dataset
Be provided under terms that allow it to be reused and redistributed
Allow universal participation so that anyone can use, reuse, and redistribute the data

Data can only be considered open when it meets all three of these standards.

One of the biggest benefits of open data is that credible databases can be used more widely. Basically, this means that all of that good data can be leveraged, shared, and combined with other data. This could have a huge impact on scientific collaboration, research advances, analytical capacity, and decision-making. But it is important to think about the individuals being represented by the public, open data, too.

Third-party data is collected by an entity that doesn’t have a direct relationship with the data. You might remember learning about this type of data earlier. For example, third parties might collect information about visitors to a certain website. Doing this lets these third parties create audience profiles, which helps them better understand user behavior and target them with more effective advertising.

Personal identifiable information (PII) is data that is reasonably likely to identify a person and make information known about them.

Sites and Resources for Open Data:

https://www.data.gov/
https://www.census.gov/data.html
https://www.opendatanetwork.com/
https://cloud.google.com/public-datasets
https://datasetsearch.research.google.com/



Week 3

Databases enable analysts to manipulate, store, and process data. This helps them search through data a lot more efficiently to get the best insights.

A relational database is a database that contains a series of tables that can be connected to show relationships. Basically, they allow data analysts to organize and link data based on what the data has in common.
In a non-relational table, you will find all of the possible variables you might be interested in analyzing all grouped together. This can make it really hard to sort through. This is one reason why relational databases are so common in data analysis: they simplify a lot of analysis processes and make data easier to find and use across an entire database.

Tables in a relational database are connected by the fields they have in common
A primary key is an identifier that references a column in which each value is unique. In other words, it's a column of a table that is used to uniquely identify each record within that table
By contrast, a foreign key is a field within a table that is a primary key in another table. A table can have only one primary key, but it can have multiple foreign keys.

Databases use a special language to communicate called a query language. Structured Query Language (SQL) is a type of query language that lets data analysts communicate with a database. So, a data analyst will use SQL to create a query to view the specific data that they want from within the larger set.

Metadata is information that's used to describe the data that's contained in something, like a photo or an email. Keep in mind that metadata is not the data itself. Instead, it's data about the data.
In data analytics, metadata helps data analysts interpret the contents of the data within a database
That's why metadata is so important when working with databases. It tells an analyst what the data is all about
As a data analyst, there are three common types of metadata that you'll come across: descriptive, structural, and administrative.

Descriptive metadata is metadata that describes a piece of data and can be used to identify it at a later point in time.
Next is structural metadata, which is metadata that indicates how a piece of data is organized and whether it's part of one or more than one data collection.
Administrative metadata is metadata that indicates the technical source of a digital asset.

In today’s digital world, metadata is everywhere, and it is becoming a more common practice to provide metadata on a lot of media and information you interact with. Here are some real-world examples of where to find metadata:

Photos
Whenever a photo is captured with a camera, metadata such as camera filename, date, time, and geolocation are gathered and saved with it.

Emails
When an email is sent or received, there is lots of visible metadata such as subject line, the sender, the recipient and date and time sent. There is also hidden metadata that includes server names, IP addresses, HTML format, and software details.

Spreadsheets and documents
Spreadsheets and documents are already filled with a considerable amount of data so it is no surprise that metadata would also accompany them. Titles, author, creation date, number of pages, user comments as well as names of tabs, tables, and columns are all metadata that one can find in spreadsheets and documents.

Websites
Every web page has a number of standard metadata fields, such as tags and categories, site creator’s name, web page title and description, time of creation and any iconography.

Digital files
Usually, if you right click on any computer file, you will see its metadata. This could consist of file name, file size, date of creation and modification, and type of file.

Books
Metadata is not only digital. Every book has a number of standard metadata on the covers and inside that will inform you of its title, author’s name, a table of contents, publisher information, copyright description, index, and a brief description of the book’s contents.

Metadata creates a single source of truth by keeping things consistent and uniform
After all, data that's uniform can be organized, classified, stored, accessed, and used effectively.

Metadata also makes data more reliable by making sure it's accurate, precise, relevant, and timely
This also makes it easier for data analysts to identify the root causes of any problems that might pop up.

A metadata repository is a database specifically created to store metadata. Metadata repositories can be stored in a physical location, or they can be virtual, like data that exists in the cloud. These repositories describe where metadata came from, keep it in an accessible form so it can be used quickly and easily, and keep it in a common structure for everyone who may need to use it.

Metadata repositories make it easier and faster to bring together multiple sources for data analysis. They do this by describing the state and location of the metadata, the structure of the tables inside, and how data flows through the repository.
They even keep track of who accesses the metadata and when

On the other hand, metadata is stored in a single, central location and it gives the company standardized information about all of its data.
This is done in two ways.

First, metadata includes information about where each system is located and where the data sets are located within those systems.
Second, the metadata describes how all of the data is connected between the various systems.

Data governance is a process to ensure the formal management of a company’s data assets.
This gives an organization better control of their data and helps a company manage issues related to data security and privacy, integrity, usability, and internal and external data flows.

These are metadata specialists, and they organize and maintain company data, ensuring that it's of the highest possible quality. These people create basic metadata identification and discovery information, describe the way different data sets work together, and explain the many different types of data resources. Metadata specialists also create very important standards that everyone follows and the models used to organize the data.
They're passionate about making data accessible by sharing with colleagues and other stakeholders.

From external source to a spreadsheet
When you work with spreadsheets, there are a few different ways to import data. This reading covers how you can import data from external sources, specifically:

Other spreadsheets
CSV files
HTML tables (in web pages)

In a lot of cases, you might have an existing spreadsheet open and need to add additional data from another spreadsheet.

In Google Sheets, you can use the IMPORTRANGE function. It enables you to specify a range of cells in the other spreadsheet to duplicate in the spreadsheet you are working in. You must allow access to the spreadsheet containing the data the first time you import the data.
In Microsoft Excel, to import data from another spreadsheet, do the following:

Step 1: Select Data from the main menu.
Step 2: Click Get Data, select From File, and then select From Workbook.
Step 3: Browse for and select the spreadsheet file and then click Import.
Step 4: In the Navigator, select which worksheet to import.
Step 5: Click Load to import all the data in the worksheet; or click Transform Data to open the Power Query Editor to adjust the columns and rows of data you want to import.
Step 6: If you clicked Transform Data, click Close & Load and then select one of the two options:
    Close & Load - import the data to a new worksheet
    Close & Load to... - import the data to an existing worksheet


Importing data from CSV files

Google Sheets

Step 1: Open the File menu in your spreadsheet and select Import to open the Import file window.
Step 2: Select Upload and then select the CSV file you want to import.
Step 3:  From here, you will have a few options. For Import location, you can choose to replace the current spreadsheet, create a new spreadsheet, insert the CSV data as a new sheet, add the data to the current spreadsheet, or replace the data in a specific cell. The data will be inserted as plain text only if you uncheck the Convert text to numbers, dates, and formulas checkbox, which is the default setting. Sometimes a CSV file uses a separator like a semi-colon or even a blank space instead of a comma. For Separator type, you can select Tab or Comma, or select Custom to enter another character that is being used as the separator.
Step 4: Select Import data. The data in the CSV file will be loaded into your sheet, and you can begin using it!

Microsoft Excel

Step 1: Open a new or existing spreadsheet

Step 2: Click Data in the main menu and select the From Text/CSV option.

Step 3: Browse for and select the CSV file and then click Import.

Step 4: From here, you will have a few options. You can change the delimiter from a comma to another character such as a semicolon. You can also turn automatic data type detection on or off. And, finally, you can transform your data by clicking Transform Data to open the Power Query Editor.

Step 5: In most cases, accept the default settings in the previous step and click Load to load the data in the CSV file to the spreadsheet. T​he data in the CSV file will be loaded into the spreadsheet, and you can begin working with the data.


Importing HTML tables from web pages

In Google Sheets, you can use the IMPORTHTML function. It enables you to import the data from an HTML table (or list) on a web page.
In Microsoft Excel, you can import data from web pages using the From Web option:

Step 1: Open a new or existing spreadsheet.
Step 2: Click Data in the main menu and select the From Web option.
Step 3: Enter the URL and click OK.
Step 4: In the Navigator, select which table to import.
Step 5: Click Load to load the data from the table into your spreadsheet.

Sorting and filtering the data in a spreadsheet helps us customize the way data is presented. They can also organize data so analysts can zoom in on the pieces that matter. Think of it like a magnifying glass for our data

Sorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
Sorting can be done across all of a spreadsheet or just in a single column or table. You can also sort by multiple variables. For instance, if our data set contains both city and state fields, we can sort first by city and then by state.

Anytime you're sorting data, it's always a good idea to freeze the header row first. To do this, we'll highlight the row. Then from the view menu, choose freeze and one row.
Multiple criteria sorting is another very useful data analysis tool.

But sometimes data analysts want to isolate a particular piece of information. To do this, they use a filter. Filtering means showing only the data that meets a specific criteria while hiding the rest
A filter simplifies a spreadsheet by only showing us the information we need


*****DATA CLEANING ACTIVITY SPREADSHEETS (Google Sheets)*****

It’s important to make sure your data is clean so that your eventual analysis will be correct. The first thing to do is check the values in the columns most relevant to your analysis and find out if there is anything for you to clean. In this example, the superintendent’s main objective is to determine what factors drive student performance. To begin answering this question, the columns you want to focus on first are school, age, reason, Medu, Fedu. You can use sorting and filtering to clean the data in each of these columns.

Sorting data

Because you have data from two schools, Gabriel Pereira (GP) and Mousinho da Silveira (MS), you can start by sorting the data by school. Then, you can also sort by age to discover the age ranges of the students for each school. Sorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
To start, rename your spreadsheet. In the upper left corner, click Untitled Spreadsheet and enter a new name. You can use the name student_performance_data or a similar name that describes the data your spreadsheet contains.
Now, sort by school. Because you want to sort on multiple columns, you need to select all the data in your spreadsheet. Click the blank rectangle above row 1 and to the left of column A. This lets you select all the data on your sheet.
Next, from the menu bar, select Data, then Sort range. (Note: For some versions of Google Sheets, the selection Advanced range sorting options may appear on the Data drop-down menu instead of Sort range).
In the pop-up window, select Data has header row. Now you can choose specific column headers to sort by.
In the Sort by dropdown, choose the header school. Then, click A → Z to sort in ascending order.
You also want to sort for age. Before you can sort by age, you need to click Add another sort column to choose a second column header.
In the Sort by dropdown, choose the header age. This time, click Z → A to sort in descending order. This way, the oldest students will be listed first.
Once both selections have been made, click Sort.

Now, If you scroll through the data, you’ll notice that the age range of the students at Gabriel Pereira (GP) is 15-22 years, and the age range of the students at Mousinho da Silveira (MS) is 15-20 years. It appears that both schools have similar age ranges, but the GP school has students that are a little older.
By sorting the data, you’ve discovered a potential problem with the data. Because this dataset represents high school student achievement, any age older than 18 may indicate that a mistake was made when entering that student's age. You now know what age data may need to be researched and corrected. Your next step is to ask the superintendent about the legitimate age range for students in public high school. Then, you’ll know what age data is incorrect and should be removed.

The superintendent tells you that the maximum age limit for which public education is provided is 19 years old and that the age range should be 15-19 for both schools. Any student outside this age range should be deleted from the dataset.
To clean your data, you need to remove the ages 20, 21, and 22 from your dataset. You can start by applying a filter to the age column. Filtering is the process of showing only the data that meets a specified criteria while hiding the rest. Filtering makes it easier to find data that you need.

First, apply a filter to the age column. Select the age column by clicking the letter at the top of the column (C).
Then, from the menu bar, select Data, then Create a filter.
You can now inspect the values in the age column by going to the top of the column and clicking the Filter icon ().
In Google Sheets, there are nine possible values for the field (15, 16, 17, 18, 19, 20, 21, and 22). You may notice that all the values have check marks. Filter this column for the values you want to select by unchecking all the other values (15, 16, 17, 18, and 19).
Then, click OK. This will single out the rows that contain the ages 20, 21, and 22. After you apply the filter, there should be nine such rows (seven for the GP school and two for the MS school).
To delete the nine rows, first select them by clicking their row numbers.
Then, from the menu bar, select Edit and Delete selected rows.
Click the Filter icon at the top of the age column to inspect the values once again. Now that you’ve removed the three incorrect ages (20, 21, and 22), there are five ages remaining (15, 16, 17, 18, and 19). The remaining ages are legitimate and can be used for analysis.
Finally, turn off the filter. From the menu bar, select Data and Turn off filter.

Filling in missing data

Filling in missing data is an important part of data cleaning. It’s your job to fill in these blank spaces in your data with accurate values.
The superintendent wants to know what factors influence student performance, and a student’s reason for choosing a specific school will be important to know for analysis. The reason column shows the main reason a student chose to enroll in a specific school, according to their survey response: for example, because of the school’s reputation, or because it offers certain courses,  etc. So, you need to make sure the reason column is complete and without blanks.

Start by applying a filter across the entire spreadsheet. Click on any cell in the sheet. Then, from the menu bar, select Data and Create a filter.
All the cells are now highlighted, and there are filters at the top of every column containing data. Click the Filter icon on the reason column (K).
You may notice that the data values in the reason column include blanks. Filter this column for blanks by unchecking all the other values (course, home, reputation).
Then, click OK. Now, your sheet shows all the blank rows in the reason column.
To clean your data, you need to find a good way to fill in these missing values. In this case, you cannot know what each missing value should be (that is, without a new survey, you can’t discover each student’s reason for choosing a specific school). So, you can replace the missing values with the value none_given. To do this while the column is still filtered for blanks, type none_given in the first empty cell (K2). Then, press Enter.
Select cell K2 again. A small blue square, known as the fill handle, appears in the bottom-right corner of the cell. Double click the fill handle to fill all the other blank cells with the value none_given.
Finally, turn off the filter. From the menu bar, select Data and Turn off filter. If you scroll down the reason column, you should find that the value none_given has replaced all the blanks in the reason column.


Converting data

During the data analysis process, it's sometimes necessary to change text data (words) to numeric data (numbers). For example, some statistical packages like those used to perform machine learning will only accept numeric data values as input.
In this case, the superintendent wants to know if a parent’s education level is a significant factor in student performance. The relevant data is in the Medu and Fedu columns--which, respectively, refer to the level of education of a student’s mother and father. Currently, the data is in text format. For the purposes of analysis, it will be useful to know the average education level of each student’s parents. To make this calculation, you first need to convert the data in the Medu and Fedu columns to number format.
To do this, you can match specific number values to the text data in each column. Start with the Medu column. If you click on the Filter icon at the top of the Medu column (G), you’ll notice the column contains the text data shown in the table below. You can use the following numeric codes for each piece of text data
To start, remove the filter from the Medu column.
Next, select the unfiltered Medu column data by clicking its column letter (G).
Then, from the menu bar, select Edit, then Find and replace.
Fill in the popup window for the none value. Next to Find, type none. Next to Replace with, type 0. Check the box next to Match entire cell contents.
Then, click Replace all.
While still in the popup window, repeat this process (steps 4-5) for the other four educational levels: primary education (4th grade), 5th to 9th grade, secondary education, and higher education.
After replacing all five educational levels with numeric values, click Done to close the pop-up window.
Check out your spreadsheet. All the cells in the Medu column now display numeric values.
Change the text data in the Fedu column (H) in the same way.


Now we're going to explore the different account tiers that BigQuery offers, so you know how to choose the right one for your needs, and how you can access them. BigQuery is offered to you at no charge. There are paid options available, but we won't need them for the activities in this course. Instead, we're going to talk about two account types: sandbox, and free trial.

A Sandbox account is available at no charge and anyone with a Google account can log in and use it. There are a couple of limitations to this account type. For example, you get a maximum of 12 projects at a time.
It also doesn't allow you to insert new records to a database or update the field values of existing records
Before that though, we should talk about the other way to use BigQuery without charges. The Google Cloud free trial. The free trial gives you access to more of what BigQuery has to offer with fewer overall limitations. The free trial offers $300 in credit for use in Google Cloud during the first 90 days. You won't get anywhere near that spending limit if you just use the BigQuery console to practice SQL queries. After you spend the $300 credit or after 90 days, your free trial will expire and you will need to personally select to upgrade to a paid account to keep working in Google Cloud.

First, we'll go to the BigQuery sandbox documentation page. Then go to the upper right corner and log in to whichever Google account you want to use for the BigQuery sandbox account. Then we'll select the "Go to BigQuery" button on the documentation page. This gives us a drop-down to select a country and to read the terms of service agreement. This will bring us to the SQL workspace, which we'll be using an upcoming activities. Choose "Create Project" and name the project and give it an ID. Choose "Create," and then "Done.


How to get to the BigQuery console
In your browser, go to console.cloud.google.com/bigquery.

Note: Going to console.cloud.google.com in your browser takes you to the main dashboard for the Google Cloud Platform. To navigate to BigQuery from the dashboard, do the following:

Click the Navigation menu icon (Hamburger icon) in the banner.
Scroll down to the BIG DATA section.
Click BigQuery and select SQL workspace.

(Optional) Explore a BigQuery public dataset

Click ADD DATA in the Explorer.
Select Explore public datasets.
You can search for a dataset by topic; for example, you can enter bike share in the search bar to get the results below. Click a dataset for additional details.
Click VIEW DATASET.
Click QUERY TABLE to get started.
Modify the query to specify what you want to run. If you add * between SELECT and FROM, the query will return data from all columns in the data table.
After you add * after SELECT, click RUN to run the query
You can drag the results pane up to resize it and see more results.
The results from the query are in the Results tab. Click the Job Information, JSON, and Execution details tabs to view other information related to the query.

(Optional) Upload a CSV file to BigQuery
Download a dataset as a CSV file
Click the project that you want to add the dataset into.
Click the Actions icon (three vertical dots) next to your project and select Create dataset to open the Create dataset options.
Enter a name for the Dataset ID. In this case, we'll name it 'medical_doctors' (your dataset will be named something else depending on the data contained in the CSV file).
Click Create dataset.
Click Go to dataset to navigate to the new dataset that you just created.
To prepare to upload the data in the CSV file, click Create table
Click the drop-down menu under Source.
Select Upload to upload the CSV file from your computer or device.
Click Browse.
Select your downloaded CSV file from your device (your file path will vary). After you select your CSV file, the file format will automatically change to CSV.
Enter a name for your table. In this case, we'll name ours 'doctors_data' (your table will be named something else depending on the dataset).
Check the Schema and input parameters check box under Auto detect.
Click Create table. This uploads the data in the CSV file into the table.
Click the name of your dataset under your project name - in our example, click "medical_doctors."
Click doctors_data.
Click the Query table icon.
Click within the editor box for the query.
Click RUN to run the query.
Click and drag up to resize the panel to see more results.


So a data analyst will use SQL to create a query to view the specific data that they want from within the larger set.
We've learned that a database is a collection of data stored in a computer system
And that SQL stands for Structured Query Language.
Data analysts write queries in order to get data from these tables.

****Big Query Practice*******

Explore BigQuery

Open your console

1. Log in to BigQuery.
2. Then, click the Go to console button on the BigQuery homepage. This will open a new tab with your console.
Take a moment to explore your console. On the left side, you will find the Explorer menu; this includes a search bar you can use to find resources, pinned projects, and the + ADD DATA button. On the right side, you will find the Query Editor. This is where you will input queries and view datasets. You can also find your Job History, Query History, and Saved Queries here.

Access public data in BigQuery
1. Click on the + ADD DATA button in the Explorer menu and select Explore public datasets. This will open a new menu where you can search public datasets that are already available through Google Cloud.
2. In the dataset menu you just opened, type london bicycle in the search box at the top; this will return the London Bicycle Hires dataset from the Greater London Authority. Click the dataset for more details.
3. From the dataset information page, click the blue VIEW DATASET button. This will open your console in a new tab with this dataset loaded.

You'll notice that bigquery-public-data is now pinned in your Explorer menu. You can now explore and query these public datasets.

Follow these steps to find and pin the bigquery-public-data if you do not have it pinned.

Navigate to the Explorer menu in BigQuery.
Type the word public in the search box and enter.
Click "Broaden search to all projects".
Find the bigquery-public-data and pin it.

4. Click on the arrow next to bigquery-public-data and scroll down the list of public datasets until you find the london_bicycles data. When you click on the dataset, it will list two tables. Click on cycle_hire.

This will pull up a new tab in your Query Editor with information about the table schema.

5. After checking out the table schema, you can take a peek into what data the cycle_hire table contains by clicking on the Preview tab. This will give you a better idea of what kind of data you’ll be working with.

Once you have finished previewing the data, you can write a query!

Query your data

So far, you’ve learned three basic parts of a query: SELECT, FROM, and WHERE. As a refresher, here are what those basic parts represent in the query:

SELECT is the section of a query that indicates what data you want SQL to return to you
FROM is the section of a query that indicates which table the desired data comes from.
WHERE is the section of a query that indicates any filters you’d like to apply to your dataset

*****Create a Custom Table BigQuery*****
Step 2: Create a dataset
Before you can upload your txt file and create a table to query, you will need to create a dataset to upload your data into and store your tables.
1. Go to the Explorer pane in your workspace and click the three dots next to your pinned project to open a menu. From here, select Create dataset.
2. This will open the Create dataset menu on the right side of your console. This is where you will fill out some information about the dataset. You will input the Dataset ID as babynames and set the Data location to United States (US). Once you have finished filling out this information, you can click the blue CREATE DATASET button at the bottom of the menu.

Step 3: Create table
Now that you have a custom dataset stored in your project space, this is where you will add your table.
1. From the babynames dataset, click the CREATE TABLE button. This will open another menu on the right side of your console.
2. In the Source section, you will select the Upload option under Create table from. Then you will click the Browse button to open your files. Find and open the yob2014.txt file. Set the file format to .csv. In the Destination section, name your table as names_2014. Under Schema, select Edit as text and input the following code: name:string,gender:string,count:integer. This will establish the data types of the three columns in the table. Leave the other parameters as they are, and select Create table
3. Once you have created your table, it will appear in your Explorer pane under the dataset you created earlier.

Click on the table to open it in your workspace. Here, you can check the table schema. Then, go to the Preview tab to explore your data. The table should have three columns: name, gender, and count.

Query your custom table
Click COMPOSE NEW QUERY to start a new query for this table. Then copy and paste this code:

SELECT
  name,
  count
FROM
  `babynames.names_2014`
WHERE
  gender = 'M'
ORDER BY
  count DESC
LIMIT
  5

This query SELECTs the name and count columns from the names_2014 table. Using the WHERE clause, you are filtering for a specific gender for your results. Then, you’re sorting how you want your results to appear with ORDER BY. Because you are ordering by the count in descending order, you will get names and the corresponding count from largest to smallest. And finally, LIMIT tells SQL to only return the top five most popular names and their counts.

Once you have input this in your console, select RUN to get your query results.


In-depth guide: SQL best practices

Capitalization and case sensitivity
With SQL, capitalization usually doesn’t matter. You could write SELECT or select or SeLeCT. They all work! But if you use capitalization as part of a consistent style your queries will  look more professional.
To write SQL queries like a pro, it is always a good idea to use all caps for clause starters (e.g., SELECT, FROM, WHERE, etc.). Functions should also be in all caps (e.g., SUM()). Column names should be all lowercase (refer to the section on snake_case later in this guide). Table names should be in CamelCase (refer to the section on CamelCase later in this guide). This helps keep your queries consistent and easier to read while not impacting the data that will be pulled when you run them. The only time that capitalization does matter is when it is inside quotes (more on quotes below).
Vendors of SQL databases may use slightly different variations of SQL. These variations are called SQL dialects. Some SQL dialects are case sensitive. BigQuery is one of them. Vertica is another. But most, like MySQL, PostgreSQL, and SQL Server, aren’t case sensitive. This means if you searched for country_code = ‘us’, it will return all entries that have 'us', 'uS', 'Us', and 'US'. This isn’t the case with BigQuery. BigQuery is case sensitive, so that same search would only return entries where the country_code is exactly 'us'. If the country_code is 'US', BigQuery wouldn’t return those entries as part of your result.

For the most part, it also doesn’t matter if you use single quotes ' ' or double quotes " " when referring to strings. For example, SELECT is a clause starter. If you put SELECT in quotes like 'SELECT' or "SELECT", then SQL will treat it as a text string. Your query will return an error because your query needs a SELECT clause.

But there are two situations where it does matter what kind of quotes you use:

When you want strings to be identifiable in any SQL dialect
When your string contains an apostrophe or quotation marks

Within each SQL dialect there are rules for what is accepted and what isn’t. But a general rule across almost all SQL dialects is to use single quotes for strings. This helps get rid of a lot of confusion. So if we want to reference the country US in a WHERE clause (e.g., country_code = 'US'), then use single quotes around the string 'US'.
Use a double quote for a string which contains an apostrophe


Comments as reminders
As you get more comfortable with SQL, you will be able to read and understand queries at a glance. But it never hurts to have comments in the query to remind yourself of what you are trying to do. And if you share your query, it also helps others understand it.
You can use # in place of the two dashes, --, in the above query but keep in mind that # isn’t recognized in all SQL dialects (MySQL doesn’t recognize #). So it is best to use -- and be consistent with it. When you add a comment to a query using --, the database query engine will ignore everything in the same line after --. It will continue to process the query starting on the next line.

Snake_case names for columns
CamelCase names for tables

Indentation
As a general rule, you want to keep the length of each line in a query <= 100 characters. This makes your queries easy to read. For example, check out this query with a line with >100 characters:

Multi-line comments
If you make comments that take up multiple lines, you can use -- for each line. Or, if you have more than two lines of comments, it might be cleaner and easier is to use /* to start the comment and */ to close the comment. For example, you can use the -- method like below:

SQL text editors
When you join a company, you can expect each company to use their own SQL platform and SQL dialect. The SQL platform they use (e.g., BigQuery, MySQL, or SQL Server) is where you will write and run your SQL queries. But keep in mind that not all SQL platforms provide native script editors to write SQL code. SQL text editors give you an interface where you can write your SQL queries in an easier and color-coded way. In fact, all of the code we have been working with so far was written with an SQL text editor!

In BigQuery, a data set (public or one you create) is a database of tables, where each table has data




Week 4

Best practices for file naming conventions
Review the following file naming recommendations:

Work out and agree on file naming conventions early on in a project to avoid renaming files again and again.
Align your file naming with your team's or company's existing file-naming conventions.
Ensure that your file names are meaningful; consider including information like project name and anything else that will help you quickly identify (and use) the file for the right purpose.
Include the date and version number in file names; common formats are YYYYMMDD for dates and v## for versions (or revisions).
Create a text file as a sample file with content that describes (breaks down) the file naming convention and a file name that applies it.
Avoid spaces and special characters in file names. Instead, use dashes, underscores, or capital letters. Spaces and special characters can cause errors in some applications.

Best practices for keeping files organized
Remember these tips for staying organized as you work with files:

Create folders and subfolders in a logical hierarchy so related files are stored together.
Separate ongoing from completed work so your current project files are easier to find. Archive older files in a separate folder, or in an external storage location.
If your files aren't automatically backed up, manually back them up often to avoid losing important work.


When you use consistent guidelines that describe the content, date, or version of a file and its name, you're using file naming conventions. As we've already discovered, these file naming conventions help us organize, access, process, and analyze our data. So here are some general tips on creating file naming conventions that are both logical and functional.

Data security means protecting data from unauthorized access or corruption by putting safety measures in place. Usually the purpose of data security is to keep unauthorized users from accessing or viewing sensitive data.
Encryption uses a unique algorithm to alter data and make it unusable by users and applications that don’t know the algorithm. This algorithm is saved as a “key” which can be used to reverse the encryption; so if you have the key, you can still use the data in its original form.
Tokenization replaces the data elements you want to protect with randomly generated data referred to as a “token.” The original data is stored in a separate location and mapped to the tokens. To access the complete original data, the user or application needs to have permission to use the tokenized data and the token mapping. This means that even if the tokenized data is hacked, the original data is still safe and secure in a separate location.



Week 5

There are lots of different professional sites that you can take advantage of as you start building your own online presence. For now though, we'll focus on LinkedIn and GitHub


Glossary Terms:

Access control: Features such as password protection, user permissions, and encryption that are used to protect a spreadsheet
Action-oriented question: A question whose answers lead to change
Administrative metadata: Metadata that indicates the technical source of a digital asset
Agenda: A list of scheduled appointments
Algorithm: A process or set of rules followed for a specific task
Analytical skills: Qualities and characteristics associated with using facts to solve problems
Analytical thinking: The process of identifying and defining a problem, then solving it by using data in an organized, step-by-step manner
Attribute: A characteristic or quality of data used to label a column in a table
Audio file: Digitized audio storage usually in an MP3, AAC, or other compressed format
AVERAGE: A spreadsheet function that returns an average of the values from a selected range
Bad data source: A data source that is not reliable, original, comprehensive, current, and cited (ROCCC)
Bias: A conscious or subconscious preference in favor of or against a person, group of people, or thing
Big data: Large, complex datasets typically involving long periods of time, which enable data analysts to address far-reaching business problems
Boolean data: A data type with only two possible values, usually true or false
Borders: Lines that can be added around two or more cells on a spreadsheet
Business task: The question or problem data analysis resolves for a business
Cell reference: A cell or a range of cells in a worksheet typically used in formulas and functions
Cloud: A place to keep data online, rather than a computer hard drive
Confirmation bias: The tendency to search for or interpret information in a way that confirms pre-existing beliefs
Consent: The aspect of data ethics that presumes an individual’s right to know how and why their personal data will be used before agreeing to provide it
Context: The condition in which something exists or happens
Continuous data: Data that is measured and can have almost any numeric value
Cookie: A small file stored on a computer that contains information about its users
COUNT: A spreadsheet function that counts the number of cells in a range that meet a specified criteria
CSV (comma-separated values) file: A delimited text file that uses a comma to separate values
Currency: The aspect of data ethics that presumes individuals should be aware of financial transactions resulting from the use of their personal data and the scale of those transactions
Dashboard: A tool that monitors live, incoming data
Data: A collection of facts
Data analysis: The collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making
Data analysis process: The six phases of ask, prepare, process, analyze, share, and act whose purpose is to gain insights that drive informed decision-making
Data analyst: Someone who collects, transforms, and organizes data in order to draw conclusions, make predictions, and drive informed decision-making
Data analytics: The science of data
Data anonymization: The process of protecting people's private or sensitive data by eliminating identifying information
Data bias: When a preference in favor of or against a person, group of people, or thing systematically skews data analysis results in a certain direction
Data design: How information is organized
Data-driven decision-making: Using facts to guide business strategy
Data ecosystem: The various elements that interact with one another in order to produce, manage, store, organize, analyze, and share data
Data element: A piece of information in a dataset
Data ethics: Well-founded standards of right and wrong that dictate how data is collected, shared, and used
Data governance: A process for ensuring the formal management of a company’s data assets
Data-inspired decision-making: Exploring different data sources to find out what they have in common
Data interoperability: The ability to integrate data from multiple sources and a key factor leading to the successful use of open data among companies and governments
Data life cycle: The sequence of stages that data experiences, which include plan, capture, manage, analyze, archive, and destroy
Data model: A tool for organizing data elements and how they relate to one another
Data privacy: Preserving a data subject’s information any time a data transaction occurs
Data science: A field of study that uses raw data to create new ways of modeling and understanding the unknown
Data security: Protecting data from unauthorized access or corruption by adopting safety measures
Data strategy: The management of the people, processes, and tools used in data analysis
Data type: An attribute that describes a piece of data based on its values, its programming language, or the operations it can perform
Data visualization: The graphical representation of data
Database: A collection of data stored in a computer system
Dataset: A collection of data that can be manipulated or analyzed as one unit
Descriptive metadata: Metadata that describes a piece of data and can be used to identify it at a later point in time
Digital photo: An electronic or computer-based image usually in BMP or JPG format
Discrete data: Data that is counted and has a limited number of values
Equation: A calculation that involves addition, subtraction, multiplication, or division (also called a math expression)
Ethics: Well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness, or specific virtues
Experimenter bias: The tendency for different people to observe things differently (Refer to Observer bias)
External data: Data that lives and is generated outside of an organization
Fairness: A quality of data analysis that does not create or reinforce bias
Field: A single piece of information from a row or column of a spreadsheet; in a data table, typically a column in the table
Fill handle: A box in the lower-right-hand corner of a selected spreadsheet cell that can be dragged through neighboring cells in order to continue an instruction
Filtering: The process of showing only the data that meets a specified criteria while hiding the rest
First-party data: Data collected by an individual or group using their own resources
Foreign key: A field within a database table that is a primary key in another table (Refer to primary key)
Formula: A set of instructions used to perform a calculation using the data in a spreadsheet
FROM: The section of a query that indicates where the selected data comes from
Function: A preset command that automatically performs a specified process or task using the data in a spreadsheet
Gap analysis: A method for examining and evaluating the current state of a process in order to identify opportunities for improvement in the future
General Data Protection Regulation of the European Union (GDPR): Policy-making body in the European Union created to help protect people and their data
Geolocation: The geographical location of a person or device by means of digital information
Good data source: A data source that is reliable, original, comprehensive, current, and cited (ROCCC)
Header: The first row in a spreadsheet that labels the type of data in each column
Internal data: Data that lives within a company’s own systems
Interpretation bias: The tendency to interpret ambiguous situations in a positive or negative way
Leading question: A question that steers people toward a certain response
Long data: A dataset in which each row is one time point per subject, so each subject has data in multiple rows
Math expression: A calculation that involves addition, subtraction, multiplication, or division (also called an equation)
Math function: A function that is used as part of a mathematical formula
MAX: A spreadsheet function that returns the largest numeric value from a range of cells
Measurable question: A question whose answers can be quantified and assessed
Mentor: Someone who shares knowledge, skills, and experience to help another grow both professionally and personally
Metadata: Data about data
Metadata repository: A database created to store metadata
Metric: A single, quantifiable type of data that is used for measurement
Metric goal: A measurable goal set by a company and evaluated using metrics
MIN: A spreadsheet function that returns the smallest numeric value from a range of cells
Naming conventions: Consistent guidelines that describe the content, creation date, and version of a file in its name
Networking: Building relationships by meeting people both in person and online
Nominal data: A type of qualitative data that is categorized without a set order
Normalized database: A database in which only related data is stored in each table
Notebook: An interactive, editable programming environment for creating data reports and showcasing data skills
Observation: The attributes that describe a piece of data contained in a row of a table
Observer bias: The tendency for different people to observe things differently (also called experimenter bias)
Open data: Data that is available to the public
Openness: The aspect of data ethics that promotes the free access, usage, and sharing of data
Operator: A symbol that names the operation or calculation to be performed
Order of operations: Using parentheses to group together spreadsheet values in order to clarify the order in which operations should be performed
Ordinal data: Qualitative data with a set order or scale
Ownership: The aspect of data ethics that presumes individuals own the raw data they provide and have primary control over its usage, processing, and sharing
Pivot chart: A chart created from the fields in a pivot table
Pivot table: A data summarization tool used to sort, reorganize, group, count, total, or average data
Pixel: In digital imaging, a small area of illumination on a display screen that, when combined with other adjacent areas, forms a digital image
Population: In data analytics, all possible data values in a dataset
Primary key: An identifier in a database that references a column in which each value is unique (Refer to foreign key)
Problem domain: The area of analysis that encompasses every activity affecting or affected by a problem
Problem types: The various problems that data analysts encounter, including categorizing things, discovering connections, finding patterns, identifying themes, making predictions, and spotting something unusual
Qualitative data: A subjective and explanatory measure of a quality or characteristic
Quantitative data: A specific and objective measure, such as a number, quantity, or range
Query: A request for data or information from a database
Query language: A computer programming language used to communicate with a database
Range: A collection of two or more cells in a spreadsheet
Record: A collection of related data in a data table, usually synonymous with row
Redundancy: When the same piece of data is stored in two or more places
Reframing: The process of restating a problem or challenge, then redirecting it toward a potential resolution
Relational database: A database that contains a series of tables that can be connected to form relationships
Relevant question: A question that has significance to the problem to be solved
Report: A static collection of data periodically given to stakeholders
Return on investment (ROI): A formula that uses the metrics of investment and profit to evaluate the success of an investment
Revenue: The total amount of income generated by the sale of goods or services
Root cause: The reason why a problem occurs
Sample: In data analytics, a segment of a population that is representative of the entire population
Sampling bias: Overrepresenting or underrepresenting certain members of a population as a result of working with a sample that is not representative of the population as a whole
Schema: A way of describing how something, such as data, is organized
Scope of work (SOW): An agreed-upon outline of the tasks to be performed during a project
Second-party data: Data collected by a group directly from its audience and then sold
SELECT: The section of a query that indicates the subset of a dataset
Small data: Small, specific data points typically involving a short period of time, which are useful for making day-to-day decisions
SMART methodology: A tool for determining a question’s effectiveness based on whether it is specific, measurable, action-oriented, relevant, and time-bound
Social media: Websites and applications through which users create and share content or participate in social networking
Sorting: The process of arranging data into a meaningful order to make it easier to understand, analyze, and visualize
Specific question: A question that is simple, significant, and focused on a single topic or a few closely related ideas
Sponsor: A professional advocate who is committed to moving forward the career of another
Spreadsheet: A digital worksheet
SQL: (Refer to Structured Query Language)
Stakeholders: People who invest time and resources into a project and are interested in its outcome
String data type: A sequence of characters and punctuation that contains textual information (also called text data type)
Structural metadata: Metadata that indicates how a piece of data is organized and whether it is part of one or more than one data collection
Structured data: Data organized in a certain format such as rows and columns
Structured Query Language: A computer programming language used to communicate with a database
Structured thinking: The process of recognizing the current problem or situation, organizing available information, revealing gaps and opportunities, and identifying options
SUM: A spreadsheet function that adds the values of a selected range of cells
Technical mindset: The ability to break things down into smaller steps or pieces and work with them in an orderly and logical way
Text data type: A sequence of characters and punctuation that contains textual information (also called string data type)
Third-party data: Data provided from outside sources who didn’t collect it directly
Time-bound question: A question that specifies a timeframe to be studied
Transaction transparency: The aspect of data ethics that presumes all data-processing activities and algorithms should be explainable and understood by the individual who provides the data
Turnover rate: The rate at which employees voluntarily leave a company
Unbiased sampling: When the sample of the population being measured is representative of the population as a whole
Unfair question: A question that makes assumptions or is difficult to answer honestly
United States Census Bureau: An agency in the U.S. Department of Commerce that serves as the nation’s leading provider of quality data about its people and economy
Unstructured data: Data that is not organized in any easily identifiable manner
Video file: A collection of images, audio files, and other data usually encoded in a compressed format such as MP4, MV4, MOV, AVI, or FLV
Visualization: (Refer to data visualization)
WHERE: The section of a query that specifies criteria that the requested data must meet
Wide data: A dataset in which every data subject has a single row with multiple columns to hold the values of various attributes of the subject
World Health Organization: An organization whose primary role is to direct and coordinate international health within the United Nations system