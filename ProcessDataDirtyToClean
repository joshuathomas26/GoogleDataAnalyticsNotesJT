Week 1

Data integrity is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle.
Data replication is the process of storing data in multiple locations.
There's also the issue of data transfer, which is the process of copying data from a storage device to memory, or from one computer to another.
The data manipulation process involves changing the data to make it more organized and easier to read. Data manipulation is meant to make the data analysis process more efficient, but an error during the process can compromise the efficiency.
Finally, data can also be compromised through human error, viruses, malware, hacking, and system failures, which can all lead to even more headaches.

Reference: Data constraints and examples
As you progress in your data journey, you'll come across many types of data constraints (or criteria that determine validity). The  table below offers definitions and examples of data constraint terms you might come across.

Data constraint:

Data type
Data range
Mandatory
Unique
Regular expression (regex) patterns
Cross-field validation
Primary-key
Set-membership
Foreign-key
Accuracy
Completeness
Consistency

Definition:

Values must be of a certain type: date, number, percentage, Boolean, etc.
Values must fall between predefined maximum and minimum values
Values can’t be left blank or empty
Values can’t have a duplicate
Values must match a prescribed pattern
Certain conditions for multiple fields must be satisfied
(Databases only) value must be unique per column
(Databases only) values for a column must come from a set of discrete values
(Databases only) values for a column must be unique values coming from a column in another table
The degree to which the data conforms to the actual entity being measured or described
The degree to which the data contains all desired components or measures
The degree to which the data is repeatable from different points of entry or collection

Examples:

If the data type is a date, a single number like 30 would fail the constraint and be invalid
If the data range is 10-20, a value of 30 would fail the constraint and be invalid
If age is mandatory, that value must be filled in
Two people can’t have the same mobile phone number within the same service area
A phone number must match ###-###-#### (no other characters allowed)
Values are percentages and values from multiple fields must add up to 100%
A database table can’t have two rows with the same primary key value. A primary key is an identifier in a database that references a column in which each value is unique. More information about primary and foreign keys is provided later in the program.
Value for a column must be set to Yes, No, or Not Applicable
In a U.S. taxpayer database, the State column must be a valid state or territory with the set of acceptable values defined in a separate States table
If values for zip codes are validated by street location, the accuracy of the data goes up.
If data for personal profiles required hair and eye color, and both are collected, the data is complete.
If a customer has the same address in the sales and repair databases, the data is consistent.

You can gain powerful insights and make accurate conclusions when data is well-aligned to business objectives. As a data analyst, alignment is something you will need to judge. Good alignment means that the data is relevant and can help you solve a business problem or determine a course of action to achieve a given business objective.

Clean data + alignment to business objective = accurate conclusions


When there is clean data and good alignment, you can get accurate insights and make conclusions the data supports.
If there is good alignment but the data needs to be cleaned, clean the data before you perform your analysis.
If the data only partially aligns with an objective, think about how you could modify the objective, or use data constraints to make sure that the subset of data better aligns with the business objective.

When you are getting ready for data analysis, you might realize you don’t have the data you need or you don’t have enough of it. In some cases, you can use what is known as proxy data in place of the real data. Think of it like substituting oil for butter in a recipe when you don’t have butter. In other cases, there is no reasonable substitute and your only option is to collect more data.

Data issue 1: no data
Possible Solutions:
Gather the data on a small scale to perform a preliminary analysis and then request additional time to complete the analysis after you have collected more data.


Examples of solutions in real life:

If you are surveying employees about what they think about a new performance and bonus plan, use a sample for a preliminary analysis. Then, ask for another 3 weeks to collect the data from all employees.
If you are analyzing peak travel times for commuters but don’t have the data for a particular city, use the data from another city with a similar size and demographic.


Data issue 2: too little data
Possible Solutions:

Do the analysis using proxy data along with actual data.
Adjust your analysis to align with the data you already have.


Examples of solutions in real life:

If you are analyzing trends for owners of golden retrievers, make your dataset larger by including the data from owners of labradors.
If you are missing data for 18- to 24-year-olds, do the analysis but note the following limitation in your report: this conclusion applies to adults 25 years and older only.



Data issue 3: wrong data, including data with errors*
Possible Solutions:

If you have the wrong data because requirements were misunderstood, communicate the requirements again.
Identify errors in the data and, if possible, correct them at the source by looking for a pattern in the errors.
If you can’t correct data errors yourself, you can ignore the wrong data and go ahead with the analysis if your sample size is still large enough and ignoring the data won’t cause systematic bias.


Examples of solutions in real life:

If you need the data for female voters and received the data for male voters, restate your needs.
If your data is in a spreadsheet and there is a conditional statement or boolean causing calculations to be wrong, change the conditional statement instead of just fixing the calculated values.
If your dataset was translated from a different language and some of the translations don’t make sense, ignore the data with bad translation and go ahead with the analysis of the other data.




Terminology:

Population
Sample
Margin of error
Confidence level
Confidence interval
Statistical significance



Definitions:

The entire group that you are interested in for your study. For example, if you are surveying people in your company, the population would be all the employees in your company.
A subset of your population. Just like a food sample, it is called a sample because it is only a taste. So if your company is too large to survey every individual, you can survey a representative sample of your population.
Since a sample is used to represent a population, the sample’s results are expected to differ from what the result would have been if you had surveyed the entire population. This difference is called the margin of error. The smaller the margin of error, the closer the results of the sample are to what the result would have been if you had surveyed the entire population.
How confident you are in the survey results. For example, a 95% confidence level means that if you were to run the same survey 100 times, you would get similar results 95 of those 100 times. Confidence level is targeted before you start your study because it will affect how big your margin of error is at the end of your study.
The range of possible values that the population’s result would be at the confidence level of the study. This range is the sample result +/- the margin of error.
The determination of whether your result could be due to random chance or not. The greater the significance, the less due to chance.


Things to remember when determining the size of your sample

When figuring out a sample size, here are things to keep in mind:

Don’t use a sample size less than 30. It has been statistically proven that 30 is the smallest sample size where an average result of a sample starts to represent the average result of a population.
The confidence level most commonly used is 95%, but 90% can work in some cases.

Increase the sample size to meet specific needs of your project:

For a higher confidence level, use a larger sample size
To decrease the margin of error, use a larger sample size
For greater statistical significance, use a larger sample size

Larger sample sizes have a higher cost



Statistical power is the probability of getting meaningful results from a test
For data analysts, your projects might begin with the test or study. Hypothesis testing is a way to see if a survey or experiment has meaningful results

For now, you should know that statistical power is usually shown as a value out of one. So if your statistical power is 0.6, that's the same thing as saying 60%. In the milk shake ad test, if you found a statistical power of 60%, that means there's a 60% chance of you getting a statistically significant result on the ad's effectiveness.

"Statistically significant" is a term that is used in statistics. If you want to learn more about the technical meaning, you can search online. But in basic terms, if a test is statistically significant, it means the results of the test are real and not an error caused by random chance.
Usually, you need a statistical power of at least 0.8 or 80% to consider your results statistically significant.


Statistical Hypothesis Testing

A statistical hypothesis test makes an assumption about the outcome, called the null hypothesis.

For example, the null hypothesis for the Pearson’s correlation test is that there is no relationship between two variables. The null hypothesis for the Student’s t test is that there is no difference between the means of two populations.

The test is often interpreted using a p-value, which is the probability of observing the result given that the null hypothesis is true, not the reverse, as is often the case with misinterpretations.

p-value (p): Probability of obtaining a result equal to or more extreme than was observed in the data.
In interpreting the p-value of a significance test, you must specify a significance level, often referred to as the Greek lower case letter alpha (a). A common value for the significance level is 5% written as 0.05.

The p-value is interested in the context of the chosen significance level. A result of a significance test is claimed to be “statistically significant” if the p-value is less than the significance level. This means that the null hypothesis (that there is no result) is rejected.

p <= alpha: reject H0, different distribution.
p > alpha: fail to reject H0, same distribution.
Where:

Significance level (alpha): Boundary for specifying a statistically significant finding when interpreting the p-value.
We can see that the p-value is just a probability and that in actuality the result may be different. The test could be wrong. Given the p-value, we could make an error in our interpretation.

There are two types of errors; they are:

Type I Error. Reject the null hypothesis when there is in fact no significant effect (false positive). The p-value is optimistically small.
Type II Error. Not reject the null hypothesis when there is a significant effect (false negative). The p-value is pessimistically large.
In this context, we can think of the significance level as the probability of rejecting the null hypothesis if it were true. That is the probability of making a Type I Error or a false positive.

Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis.

That is, the probability of a true positive result. It is only useful when the null hypothesis is rejected.

… statistical power is the probability that a test will correctly reject a false null hypothesis. Statistical power has relevance only when the null is false.

More intuitively, the statistical power can be thought of as the probability of accepting an alternative hypothesis, when the alternative hypothesis is true.

Low Statistical Power: Large risk of committing Type II errors, e.g. a false negative.
High Statistical Power: Small risk of committing Type II errors.

A contraindication is a condition that may cause a patient not to take a vaccine due to the harm it would cause them if taken. To estimate the number of possible contraindications, a data analyst proxies an open dataset from a trial of the injection version of the vaccine. The analyst selects a subset of the data with patient profiles most closely matching the makeup of the patients at the clinic.

A sample size calculator tells you how many people you need to interview (or things you need to test) to get results that represent the target population. Let’s review some terms you will come across when using a sample size calculator:

Confidence level: The probability that your sample size accurately reflects the greater population.
Margin of error: The maximum amount that the sample results are expected to differ from those of the actual population.
Population: This is the total number you hope to pull your sample from.
Sample: A part of a population that is representative of the population.
Estimated response rate: If you are running a survey of individuals, this is the percentage of people you expect will complete your survey out of those who received the survey.

In order to use a sample size calculator, you need to have the population size, confidence level, and the acceptable

Sample Size Calculator Link: https://docs.google.com/spreadsheets/d/1B1D8XgEVSTNiLg8EmXtyxZgYz_TaJBlDurIwAXPiya0/edit#gid=0

Keep in mind, the calculated sample size is the minimum number to achieve what you input for confidence level and margin of error.



Margin of error is the maximum amount that the sample results are expected to differ from those of the actual population. More technically, the margin of error defines a range of values below and above the average result for the sample. The average result for the entire population is expected to be within that range. We can better understand margin of error by using some examples below.

A/B testing (or split testing) tests two variations of the same web page to determine which page is more successful in attracting user traffic and generating revenue. User traffic that gets monetized is known as the conversion rate. A/B testing allows marketers to test emails, ads, and landing pages to find the data behind what is working and what isn’t working. Marketers use the confidence interval (determined by the conversion rate and the margin of error) to understand the results.

Calculating Margin of Error:
All you need is population size, confidence level, and sample size. In order to better understand this calculator, review these terms:

Confidence level: A percentage indicating how likely your sample accurately reflects the greater population
Population: The total number you pull your sample from
Sample: A part of a population that is representative of the population
Margin of error: The maximum amount that the sample results are expected to differ from those of the actual population

Margin of error is used to determine how close your sample’s result is to what the result would likely have been if you could have surveyed or tested the entire population.



Week 2

Dirty data is data that's incomplete, incorrect, or irrelevant to the problem you're trying to solve.
Dirty data can be the result of someone typing in a piece of data incorrectly,inconsistent formatting, blank fields; or the same piece of data being entered more than once, which creates duplicates
The most common factor is actually human error

Clean data is data that's complete, correct, and relevant to the problem you're trying to solve. When you work with clean data, you'll find that your projects go much more smoothly.

Data engineers transform data into a useful format for analysis and give it a reliable infrastructure.
This means they develop, maintain, and test databases, data processors and related systems

Data warehousing specialists develop processes and procedures to effectively store and organize data. They make sure that data is available, secure, and backed up to prevent loss

A null is an indication that a value does not exist in a data set. Note that it's not the same as a zero. In the case of a survey, a null would mean the customers skipped that question. A zero would mean they provided zero as their response.
You could either filter them out and communicate that you now have a smaller sample size, or you can keep them in and learn from the fact that the customers did not provide responses.

Duplicate data

Description: Any data record that shows up more than once

Possible causes: Manual data entry, batch data imports, or data migration

Potential harm to businesses: Skewed metrics or analyses, inflated or inaccurate counts or predictions, or confusion during data retrieval


Outdated Data

Description:Any data that is old which should be replaced with newer and more accurate information

Possible Causes: People changing roles or companies, or software and systems becoming obsolete

Potential harm to businesses: Inaccurate insights, decision-making, and analytics


Incomplete Data

Description: Any data that is missing important fields

Possible Causes: Improper data collection or incorrect data entry

Potential harm to businesses: Decreased productivity, inaccurate insights, or inability to complete essential services


Inaccurate/Incorrect Data

Description: Any data that is complete but inaccurate

Possible Causes: Human error inserted during data input, fake information, or mock data

Potential harm to businesses:Inaccurate insights or decision-making based on bad information resulting in revenue loss



Inconsistent Data:

Description: Any data that uses different formats to represent the same thing

Possible Causes: Data stored incorrectly or errors inserted during data transfer

Potential harm to businesses:Contradictory data points leading to confusion or inability to classify or segment customers


Hey there, in this video we'll focus on common issues associated with dirty data. These include spelling and other text errors, inconsistent labels, formats and field length, missing data, and duplicates. This will help you recognize problems quicker and give you the information you need to fix them when you encounter something similar during your own analysis.


Okay, now let's continue on to some other types of dirty data. The first has to do with labeling, to understand labelling imagine trying to get a computer to correctly identify panda bears among images of all different kinds of animals. You need to show the computer thousands of images of panda bears, they're all labeled as panda bears. Any incorrectly labeled picture, like the one here that's just "bear", will cause a problem.

The next cause of dirty data is having an inconsistent field length, you learned earlier that a field is a single piece of information from a row or column of a spreadsheet. Field length is a tool for determining how many characters can be keyed into a field, assigning a certain length to the fields in your spreadsheet is a great way to avoid errors.
Some spreadsheet applications have a simple way to specify field lengths and make sure users can only enter a certain number of characters into a field. This is part of data validation

Data validation is a tool for checking the accuracy and quality of data before adding or importing it. Data validation is a form of data cleaning, which you'll learn more about soon. But first you'll get familiar with more techniques for cleaning data.


As you've learned, clean data is essential to data integrity and reliable solutions and decisions. The good news is that spreadsheets have all kinds of tools you can use to get your data ready for analysis. The techniques for data cleaning will be different depending on the specific data set you're working with

Here, we'll discuss how to remove unwanted data, clean up text to remove extra spaces and blanks, fix typos, and make formatting consistent. However, before removing unwanted data, it's always a good practice to make a copy of the data set.

Removing irrelevant data takes a little more time and effort because you have to figure out the difference between the data you need and the data you don't. But believe me, making those decisions will save you a ton of effort down the road.
The next step is removing extra spaces and blanks. Extra spaces can cause unexpected results when you sort, filter, or search through your data. And because these characters are easy to miss, they can lead to unexpected and confusing results
The next data cleaning step involves fixing misspellings, inconsistent capitalization, incorrect punctuation, and other typos.
The next step is removing formatting. This is particularly important when you get data from lots of different sources. Every database has its own formatting, which can cause the data to seem inconsistent. Creating a clean and consistent visual appearance for your spreadsheets will help make it a valuable tool for you and your team when making key decisions.

Cleaning data that comes from two or more sources is very common for data analysts, but it does come with some interesting challenges

First, all the data from each organization would need to be combined using data merging. Data merging is the process of combining two or more datasets into a single dataset. This presents a unique challenge because when two totally different datasets are combined, the information is almost guaranteed to be inconsistent and misaligned.

The first question I would ask is, do I have all the data I need?
Next I would ask, does the data I need exist within these datasets?
Do the datasets need to be cleaned, or are they ready for me to use?

In both of the examples we explored here, data analysts could use either the spreadsheet tools or SQL queries to clean up, merge, and prepare the datasets for analysis.


Common mistakes to avoid

Not checking for spelling errors: Misspellings can be as simple as typing or input errors. Most of the time the wrong spelling or common grammatical errors can be detected, but it gets harder with things like names or addresses. For example, if you are working with a spreadsheet table of customer data, you might come across a customer named “John” whose name has been input incorrectly as “Jon” in some places. The spreadsheet’s spellcheck probably won’t flag this, so if you don’t double-check for spelling errors and catch this, your analysis will have mistakes in it.
Forgetting to document errors: Documenting your errors can be a big time saver, as it helps you avoid those errors in the future by showing you how you resolved them. For example, you might find an error in a formula in your spreadsheet. You discover that some of the dates in one of your columns haven’t been formatted correctly. If you make a note of this fix, you can reference it the next time your formula is broken, and get a head start on troubleshooting. Documenting your errors also helps you keep track of changes in your work, so that you can backtrack if a fix didn’t work.
Not checking for misfielded values: A misfielded value happens when the values are entered into the wrong field. These values might still be formatted correctly, which makes them harder to catch if you aren’t careful. For example, you might have a dataset with columns for cities and countries. These are the same type of data, so they are easy to mix up. But if you were trying to find all of the instances of Spain in the country column, and Spain had mistakenly been entered into the city column, you would miss key data points. Making sure your data has been entered correctly is key to accurate, complete analysis.
Overlooking missing values: Missing values in your dataset can create errors and give you inaccurate conclusions. For example, if you were trying to get the total number of sales from the last three months, but a week of transactions were missing, your calculations would be inaccurate.  As a best practice, try to keep your data as clean as possible by maintaining completeness and consistency.
Only looking at a subset of the data: It is important to think about all of the relevant data when you are cleaning. This helps make sure you understand the whole story the data is telling, and that you are paying attention to all possible errors. For example, if you are working with data about bird migration patterns from different sources, but you only clean one source, you might not realize that some of the data is being repeated. This will cause problems in your analysis later on. If you want to avoid common errors like duplicates, each field of your data requires equal attention.
Losing track of business objectives: When you are cleaning data, you might make new and interesting discoveries about your dataset-- but you don’t want those discoveries to distract you from the task at hand. For example, if you were working with weather data to find the average number of rainy days in your city, you might notice some interesting patterns about snowfall, too. That is really interesting, but it isn’t related to the question you are trying to answer right now. Being curious is great! But try not to let it distract you from the task at hand.
Not fixing the source of the error: Fixing the error itself is important. But if that error is actually part of a bigger problem, you need to find the source of the issue. Otherwise, you will have to keep fixing that same error over and over again. For example, imagine you have a team spreadsheet that tracks everyone’s progress. The table keeps breaking because different people are entering different values. You can keep fixing all of these problems one by one, or you can set up your table to streamline data entry so everyone is on the same page. Addressing the source of the errors in your data will save you a lot of time in the long run.
Not analyzing the system prior to data cleaning: If we want to clean our data and avoid future errors, we need to understand the root cause of your dirty data. Imagine you are an auto mechanic. You would find the cause of the problem before you started fixing the car, right? The same goes for data. First, you figure out where the errors come from. Maybe it is from a data entry error, not setting up a spell check, lack of formats, or from duplicates. Then, once you understand where bad data comes from, you can control it and keep your data clean.
Not backing up your data prior to data cleaning: It is always good to be proactive and create your data backup before you start your data clean-up. If your program crashes, or if your changes cause a problem in your dataset, you can always go back to the saved version and restore it. The simple procedure of backing up your data can save you hours of work-- and most importantly, a headache.
Not accounting for data cleaning in your deadlines/process: All good things take time, and that includes data cleaning. It is important to keep that in mind when going through your process and looking at your deadlines. When you set aside time for data cleaning, it helps you get a more accurate estimate for ETAs for stakeholders, and can help you know when to request an adjusted ETA.


****Cleaning Data With Spreadsheets Activity******

Select and remove blank cells

The first technique we’ll use is to select and eliminate rows containing blank cells by using filters. To eliminate rows with blank cells:

1. Highlight all cells in the spreadsheet. You can highlight Columns A-H by clicking on the header of Column A, holding Shift, and clicking on the header of Column H.
2. Click on the Data tab and pick the Create a filter option. In Microsoft Excel, this is called Filter.
3. Every column now shows a green triangle in the first row next to the column title. Click the green triangle in Column B to access a new menu.
4. On that new menu, click Filter by condition and open the dropdown menu to select Is empty. Click OK.

In Excel, click the dropdown, then Filter... then make sure only (Blanks) is checked. Click OK.

5. Select all these cells and delete the rows except the row of column headers.
6. Return to the Filter by condition and return it to None. In Excel, click Clear Filter from ‘Column’.

Note: You will now notice that any row that had an empty cell in Column A will be removed (including the extra empty rows after the data).

7. Repeat this for Columns B-H.

All the rows that had blank cells are now removed from the spreadsheet.


Transpose the data

The second technique you will practice will help you convert the data from the current long format (more rows than columns) to the wide format (more columns than rows). This action is called transposing. To transpose your data:

1. Highlight and copy the data that you want to transpose including the column labels. You can do this by highlighting Columns A-H. In Excel, highlight only the relevant cells (A1-H45) instead of the headers.
2. Right-click on cell I1. This is where you want the transposed data to start.
3. Hover over Paste Special from the right-click menu. Select the Transposed option. In Excel, select the Transpose icon under the paste options.

You should now find the data transformed into the new wide format. At this point, you should remove the original long data from the spreadsheet.

4. Delete the previous long data. The easiest way to do this is to click on Column A, so the entire column is highlighted. Then, hold down the Shift key and click on Column H. You should find these columns highlighted. Right-click on the highlighted area and select Delete Columns A - H.




Get rid of extra spaces in cells with string data

Now that you have transposed the data, eliminate the extra spaces in the values of the cells.

1. Highlight the data in the spreadsheet.
2. Click on the Data tab, then hover over Data cleanup and select Trim whitespace.

In Excel, you can use the TRIM command to get rid of white spaces. In any space beneath your data (such as cell A10), type =TRIM(A1). Then, drag the bottom right corner of the cell to the bottom right to call the data without the white spaces.

Now all the extra spaces in the cells have been removed.




Change Text Lower/Uppercase/Proper Case

Next, you’ll process string data. The easiest way to clean up string data will depend on the spreadsheet program you are using. If you are using Excel, you’ll use a simple formula. If you are using Google Sheets, you can use an Add-On to do this with a few clicks. Follow the steps in the relevant section below.

Microsoft Excel

If you are using Microsoft Excel, this documentation explains how to use a formula to change the case of a text string. Follow these instructions to clean the string text and then move on to the confirmation and reflection section of this activity.

Google sheets

If you’re completing this exercise using Google Sheets, you’ll need to install an add-in that will give you the functionality needed to easily clean string data and change cases.

Google Sheets Add-on Instructions:

Click on the Add-Ons option at the top of Google Sheets.
Click on Get add-ons.
Search for ChangeCase.
Click on Install to install the add-on. It may ask you to login or verify the installation permissions.

Once you have installed the add-on successfully, you can access it by clicking on the Add-ons menu again.

Now, you can change the case of text data that shows up. To change the text in Column C to all uppercase:

1. Click on Column C. Be sure to deselect the column header, unless you want to change the case of that as well (which you don't).
2. Click on the Add-Ons tab and select ChangeCase. Select the option All uppercase. Notice the other options that you could have chosen if needed.


Delete all formatting

If you want to clear the formatting for any or all cells, you can find the command in the Format tab. To clear formatting:

1. Select the data for which you want to delete the formatting. In this case, highlight all the data in the spreadsheet by clicking and dragging over Rows 1-8.
2. Click the Format tab and select the Clear Formatting option.

In Excel, go to the Home tab, then hover over Clear and select Clear Formats.

You will notice that all the cells have had their formatting removed.


We also learned that lots of spreadsheet applications have tools that help simplify and speed up the data cleaning process. There's a lot of great efficiency tools that data analysts use all the time, such as conditional formatting, removing duplicates, formatting dates, fixing text strings and substrings, and splitting text to columns. We'll explore those in more detail now.

Conditional formatting is a spreadsheet tool that changes how cells appear when values meet specific conditions. Likewise, it can let you know when a cell does not meet the conditions you've set.
We'll use conditional formatting to highlight blank cells. That way, we know where there's missing information so we can add it to the spreadsheet. To do this, we'll start by selecting the range we want to search. For this example we're not focused on address 3 and address 5. The fields will include all the columns in our spreadsheets, except for F and H. Next, we'll go to Format and choose Conditional formatting.
Great. Our range is automatically indicated in the field. The format rule will be to format cells if the cell is empty.
Finally, we'll choose the formatting style. I'm going to pick a shade of bright pink, so my blanks really stand out.
Then click "Done," and the blank cells are instantly highlighted.

The next spreadsheet tool removes duplicates. As you've learned before, it's always smart to make a copy of the data set before removing anything. Let's do that now.
The instructor will right-click the bottom tab for the sheet and select Duplicate.
As you've learned before, it's always smart to make a copy of the data set before removing anything. Let's do that now.
To remove duplicates, go to Data and select "Remove duplicates." "Remove duplicates" is a tool that automatically searches for and eliminates duplicate entries from a spreadsheet. Choose "Data has header row" because our spreadsheet has a row at the very top that describes the contents of each column. Next, select "All" because we want to inspect our entire spreadsheet. Finally, "Remove duplicates."

Another useful spreadsheet tool enables you to make formats consistent. For example, some of the dates in this spreadsheet are in a standard date format.
This could be confusing if you wanted to analyze when association members joined, how often they renewed their memberships, or how long they've been with the association. To make all of our dates consistent, first select column J, then go to "Format," select "Number," then "Date." Now all of our dates have a consistent format

In data analytics, a text string is a group of characters within a cell, most often composed of letters
Now let's talk about Split. Split is a tool that divides a text string around the specified character and puts each fragment into a new and separate cell.
Split is helpful when you have more than one piece of data in a cell and you want to separate them out
To do this, you want each certification separated out into its own column. Right now, the certifications are separated by a comma. That's the specified text separating each item, also called the delimiter. Let's get them separated. Highlight the column, then select "Data," and "Split text to columns."
But sometimes you might need to specify what the delimiter should be. You can do that here.

Split text to columns is also helpful for fixing instances of numbers stored as text. Sometimes values in your spreadsheet will seem like numbers, but they're formatted as text. This can happen when copying and pasting from one place to another or if the formatting's wrong.
But if we select the orders column and choose "Split text to columns,"

CONCATENATE(item 1, item 2) is a function that joins multiple text strings into a single string.
COUNTIF(start: stop, condition) is a function that returns the number of cells that match a specified value.
LEN(cell) is a function that tells you the length of the text string by counting the number of characters it contains
Ex: Let's continue the function through the entire column and find out if any results are not six. But instead of manually going through our spreadsheet to search for these instances, we'll use conditional formatting. We talked about conditional formatting earlier. It's a spreadsheet tool that changes how cells appear when values meet specific conditions. Let's practice that now. Select all of column B except for the header. Then go to Format and choose Conditional formatting. The format rule is to format cells if not equal to six.
LEFT(range, number of characters wanted) is a function that gives you a set number of characters from the left side of a text string.
RIGHT(range, number of characters wanted) is a function that gives you a set number of characters from the right side of a text string.
MID(range, reference starting point, number of middle characters wanted) is a function that returns middle character
TRIM(range) is a function that removes leading, trailing, and repeated spaces in data.


In this reading, you will learn about workflow automation and how it can help you work faster and more efficiently. Basically, workflow automation is the process of automating parts of your work. That could mean creating an event trigger that sends a notification when a system is updated. Or it could mean automating parts of the data cleaning process. As you can probably imagine, automating different parts of your work can save you tons of time, increase productivity, and give you more bandwidth to focus on other important aspects of the job.

Task:

Communicating with your team and stakeholders
Presenting your findings
Preparing and cleaning data
Data exploration
Modeling the data


Can it be automated?:

No
No
Partially
Partially
Yes




Why?:

Communication is key to understanding the needs of your team and stakeholders as you complete the tasks you are working on. There is no replacement for person-to-person communications.
Presenting your data is a big part of your job as a data analyst. Making data accessible and understandable to stakeholders and creating data visualizations can’t be automated for the same reasons that communications can’t be automated.
Some tasks in data preparation and cleaning can be automated by setting up specific processes, like using a programming script to automatically detect missing values.
Sometimes the best way to understand data is to see it. Luckily, there are plenty of tools available that can help automate the process of visualizing data. These tools can speed up the process of visualizing and understanding the data, but the exploration itself still needs to be done by a data analyst.
Data modeling is a difficult process that involves lots of different factors; luckily there are tools that can completely automate the different stages.

Let's start with sorting and filtering. As you learned earlier, sorting and filtering data helps data analysts customize and organize the information the way they need for a particular project. But these tools are also very useful for data cleaning.
You might remember that sorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
For data cleaning, you can use sorting to put things in alphabetical or numerical order, so you can easily find a piece of data.
Sorting can also bring duplicate entries closer together for faster identification.

Filters, on the other hand, are very useful in data cleaning when you want to find a particular piece of information
You learned earlier that filtering means showing only the data that meets a specific criteria while hiding the rest.
When cleaning data, you might use a filter to only find values above a certain number, or just even or odd values. Again, this helps you find what you need quickly and separates out the information you want from the rest.

Pivot tables sort, reorganize, group, count, total or average data stored in the database. In data cleaning, pivot tables are used to give you a quick, clutter- free view of your data. You can choose to look at the specific parts of the data set that you need to get a visual in the form of a pivot table
Select data we want to use, insert->pivot table

VLOOKUP(data to look up cell, 'where to look!RANGE!',column number, false)  stands for vertical lookup. It's a function that searches for a certain value in a column to return a corresponding piece of information.
The dollar sign makes sure that the corresponding part of the reference remains unchanged.
You can lock just the column, just the row, or both at the same time.

The final tool we'll talk about is something called plotting. When you plot data, you put it in a graph chart, table, or other visual to help you quickly find what it looks like.
Plotting is very useful when trying to identify any skewed data or outliers. For example, if we want to make sure the price of each product is correct, we could create a chart. This would give us a visual aid that helps us quickly figure out if anything looks like an error.

Looking at data in new and creative ways helps data analysts identify all kinds of dirty data.


But it's also important to think about how your data has moved between systems and how it's evolved along it's journey to your data analysis project. To do this, data analysts use something called data mapping
Data mapping is the process of matching fields from one database to another. This is very important to the success of data migration, data integration, and lots of other data management activities. As you learned earlier, different systems store data in different ways. For example, the state field in one spreadsheet might show Maryland spelled out. But another spreadsheet might store it as MD.
Data mapping helps us note these kinds of differences so we know when data is moved and combined it will be compatible.

Starting with the first data field, we'll identified that we need to move both sets of member IDs. To define the desired format, we'll choose whether to use numbers like this spreadsheet, or email addresses like the other spreadsheet.
Next comes mapping the data. Depending on the schema and number of primary and foreign keys in a data source, data mapping can be simple or very complex. As a reminder, a schema is a way of describing how something is organized. A primary key references a column in which each value is unique and a foreign key is a field within a table that is a primary key in another table.

This brings us to the next step, transforming the data into a consistent format. This is a great time to use concatenate. As you learned before, concatenate is a function that joins together two or more text strings, which is what we did earlier with our cosmetics company example. We'll insert a new column

Now that everything's compatible, it's time to transfer the data to its destination. There's a lot of different ways to move data from one place to another, including querying, import wizards, and even simple drag and drop.

Data mapping is so important because even one mistake when merging data can ripple throughout an organization, causing the same error to appear again and again. This leads to poor results. On the other hand, data mapping can save the day by giving you a clear road map you can follow to make sure your data arrives safely at it's destination.



****Clean Data With Spreadsheets Activity****

Your goal is to fix these errors and help create a clean dataset for analysis. You can address each issue in turn.

Remove duplicates
The first step is to eliminate any duplicate entries from your dataset. As a best practice, duplicates should be removed even if they are not readily apparent.

To start, select columns A through F.
Then, in the menu bar, choose Data and Remove duplicates.
In the pop-up window, click Data has header row. You want to remove duplicate boba shop id's and boba shop names. In the Columns to analyze section, make sure the relevant columns (id, name) are selected.
Once everything has been selected, click Remove duplicates.
If done correctly, 3 duplicate rows will be found and removed and 604 rows will remain.



Correct the ratings data
Next, clean up any data that does not make sense. Yelp ratings should be less than 5 and greater than 0. Now, you will determine how many entries are inaccurate and correct them. You can use the COUNTIF function to perform this task.

The COUNTIF function quickly counts how many items in a range of cells meet a given criterion. In cell I2, type =COUNTIF(C:C,">5"). The first entry (C:C) refers to the range where you are counting the data. In this case, the range is the entire rating column (C), which contains the Yelp ratings. The second entry refers to the criterion (>5), and tells the function to count all the values greater than 5.
Press Enter. You’ll notice that the function returns a value of 9. This tells you that your dataset contains 9 entries that have a rating greater than 5.

As a data analyst, it's your job to decide what to do with incorrect values or to ask the dataset owner for advice if you’re unsure. In this case, one effective approach would be to search on Yelp for the actual ratings. For this activity, you can just replace the incorrect ratings with the number 5. An efficient way to replace the ratings is to sort the data numerically from largest to smallest rating.

3. Select columns A through F.
4. Then, from the menu bar, choose Data and Sort range.
5. In the pop-up window, check the box next to Data has header row. Sort by rating from Z →A. This way, the highest ratings will be listed first.
6. Click Sort. Check out your spreadsheet. At the start of the rating column, you should now find the 9 rows that have incorrect values (rating > 5).
7. Next, select the range of cells C2:C10. Press delete to delete the values that are greater than 5.
8. Replace all the values with the number 5. In cell C2, type 5. Then, drag the fill handle down to cell C10 to fill the remaining cells with 5.
9. After replacing the incorrect ratings with the number 5, you may notice that the new value in cell I2 is 0. The output of the COUNTIF function now reflects the changes in your dataset. This confirms that the rating column no longer contains any values greater than 5.
10. Finally, delete the formula from cell I2 since you don’t need this information anymore.



Clean up the latitude and longitude data
Next, clean up the latitude and longitude data by placing each value in a separate column. You can use the SPLIT function to accomplish this task.

1. The SPLIT function divides text around a specified character or string, and puts each fragment of text into a separate cell in the row. The SPLIT function will split the single lat-long column into two separate columns, one for latitude and the other for longitude. In cell G2, type =SPLIT(F2,"-"). The first entry (F2) refers to the cell where the text is located. The second entry (“-”) refers to the fact that you are dividing the text based on the minus sign.
2. Press Enter. The result shows each fragment of text in a different cell.
3. Select cell G2 again. In cell G2, double-click on the fill handle to split all the remaining lat-long entries.
4. Now add column headers to the two new columns (G and H). In cell G1, type lat. In cell H1, type long.
5. Next, replace the original lat-long data in column F with the new split entries in columns G and H. Select columns G and H, right-click, and choose Copy.
6. Then, select Column F, right-click, and choose Paste special and Paste values only.
7. Now the new lat column is column F, and the new long column is column G. Adjust the width of the lat column (F) to fit the data by dragging the right boundary of the column heading.
8. Next, select column H, right-click, and choose Delete column.
9. Finally, the longitude values should be negative so that they are accurate coordinates for mapping. To make the values in the long column negative, multiply them by -1. In cell H2, type =G2*-1. The asterisk is the operator for multiplication. Press Enter.
10. Still in cell H2, double-click on the fill handle to fill in the rest of the values.
11. Next, add a column header. In cell H1, type: long.
12. Now, replace the longitude data in column G with the new data in column H. Select column H, right-click, and choose Copy.
13. Select Column G, right-click, and choose Paste special and Paste values only.
14. Then, select column H, right-click, and choose Delete column.



Week 3

SQL Dialect Differences: https://www.datacamp.com/blog/sql-server-postgresql-mysql-whats-the-difference-where-do-i-start
SQL Tutorial: https://www.sqltutorial.org/what-is-sql/

Data is measured by the number of bits it takes to represent it. All information in a computer can be represented as a binary number consisting solely of 0’s and 1’s. Each 0 or 1 in a number is a bit. A bit is the smallest unit of storage in computers. Since computers work in binary (Base 2), this means that all the important numbers that differentiate between different data sizes will be powers of 2.

A byte is a collection of 8 bits. Take a moment to examine the table below to get a feel for the difference between data measurements and their relative sizes to one another.


Widely Used SQL Queries

First, I'll show you how to use the SELECT query. I've called this one out before, but now I'll add some new things for us to try out.
We can use SELECT to specify exactly what data we want to interact with in a table. If we combine SELECT with FROM, we can pull data from any table in this database as long as they know what the columns and rows are named

We can also insert new data into a database or update existing data
We can use the INSERT INTO query to put that information in
***Syntax: <> Represents user input***
INSERT INTO <table>
<(columns that you are inserting in: put correct order)>
VALUES <values you are putting into those specific columns: follow order>


Now, let's say we just need to change the address of a customer. Well, we can tell the database to update it for us.
Syntax:
UPDATE <table>
SET <change column value by setting = to new data>
WHERE <indicate specific row>

Question:

The customers table contains the following columns: CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId.

Create a query to return the LastName and Country columns from this table for only customers in Germany.

My work:

SELECT LastName, Country
FROM customers
WHERE Country = 'Germany'

If we want to create a new table for this database, we can use the CREATE TABLE IF NOT EXISTS statement.
Keep in mind, just running a SQL query doesn't actually create a table for the data we extract. It just stores it in our local memory. To save it, we'll need to download it as a spreadsheet or save the result into a new table. As a data analyst, there are a few situations where you might need to do just that.

Another good thing to keep in mind, if you're creating lots of tables within a database, you'll want to use the DROP TABLE IF EXISTS statement to clean up after yourself. It's good housekeeping

Earlier, we covered how to remove duplicates in spreadsheets using the Remove duplicates tool. In SQL, we can do the same thing by including DISTINCT in our SELECT statement
We want a list of all unique customer IDs.
To do that, we add DISTINCT to our SELECT statement by writing:
SELECT DISTINCT customer_id
FROM customer_data.customer_address.

The first function I want to show you is LENGTH, which we've encountered before.
If we already know the length our string variables are supposed to be, we can use LENGTH to double-check that our string variables are consistent

For some databases, this query is written as LEN, but it does the same thing.
We can do that by putting the LENGTH(<column where you want to see the length of every value within it>) function that we created into the WHERE clause.

The good news is that we can account for this error in our results by using the substring function in our SQL query
Use SUBSTR(<column, start letter index, end letter index>)

Finally, let's check out the TRIM function, which you've come across before.
in the WHERE clause. Here's where we'll use the TRIM function.
TRIM(<column>) = value -> so what this does is trim the whitespace of inputs in the parameter column with the specific value

Basically, CAST(<column> AS type_to_cast)  can be used to convert anything from one data type to another.
BigQuery stores numbers in a 64 bit system. The float data type is referenced as float64 in our query.
The CAST function can be used to change strings into other data types too, like date and time

Finally, to sort in descending order, we type

ORDER BY
<column> DESC (or ASC for ascending)

at the end of our query.


Next up, let's check out the CONCAT function. CONCAT lets you add strings together to create new text strings that can be used as unique keys
CONCAT(column1, column2) AS new_column


I've got one last advanced function to show you, COALESCE. COALESCE can be used to return non-null values in a list.
COALESCE(col1, col2, colN): columns where you want to get non-null values



Week 4

Verification is a critical part of any analysis project.
verification is a process to confirm that a data-cleaning effort was well-executed and the resulting data is accurate and reliable. It also involves manually cleaning data to compare your expectations with what's actually present.

The first step in the verification process is going back to your original unclean data set and comparing it to what you have now.
Another key part of verification involves taking a big-picture view of your project. This is an opportunity to confirm you're actually focusing on the business problem that you need to solve and the overall project goals and to make sure that your data is actually capable of solving that problem and achieving those goals.

Second, you need to consider the goal of the project. It's not enough just to know that your company wants to analyze customer feedback about a product. What you really need to know is that the goal of getting this feedback is to make improvements to that product.
On top of that, you also need to know whether the data you've collected and cleaned will actually help your company achieve that goal.

And third, you need to consider whether your data is capable of solving the problem and meeting the project objectives. That means thinking about where the data came from and testing your data collection and cleaning processes.

Asking a teammate to review your data from a fresh perspective and getting feedback from others is very valuable in this stage.


Spreadsheet Tool Find and Replace for Misspelling
The first is using Find and replace. Find and replace is a tool that looks for a specified search term in a spreadsheet and allows you to replace it with something else. We'll choose Edit. Then Find and replace. We're trying to find P-L-O-S, the misspelling of "plus" in the supplier's name. In some cases you might not want to replace the data. You just want to find something. No problem. Just type the search term, leave the rest of the options as default and click "Done." But right now we do want to replace it with P-L-U-S. We'll type that in here. Then click "Replace all" and "Done."


Finding Errors in Data using Pivot Tables: COUNTA Spreadsheet Function
COUNTA counts the total number of values within a specified range.
Note that there's also function called COUNT, which only counts the numerical values within a specified range.


SQL Mispellings
This is also useful practice when querying a database. If you're working in SQL, you can address misspellings using a CASE statement.
CASE
    WHEN column = 'incorrect_value' THEN 'correct_value'
    ELSE column
    END AS new_name_alias

As I mentioned, a CASE statement can cover multiple cases.

Correct the most common problems
Make sure you identified the most common problems and corrected them, including:

Sources of errors: Did you use the right tools and functions to find the source of the errors in your dataset?
Null data: Did you search for NULLs using conditional formatting and filters?
Misspelled words: Did you locate all misspellings?
Mistyped numbers: Did you double-check that your numeric data has been entered correctly?
Extra spaces and characters: Did you remove any extra spaces or characters using the TRIM function?
Duplicates: Did you remove duplicates in spreadsheets using the Remove Duplicates function or DISTINCT in SQL?
Mismatched data types: Did you check that numeric, date, and string data are typecast correctly?
Messy (inconsistent) strings: Did you make sure that all of your strings are consistent and meaningful?
Messy (inconsistent) date formats: Did you format the dates consistently throughout your dataset?
Misleading variable labels (columns): Did you name your columns meaningfully?
Truncated data: Did you check for truncated or missing data that needs correction?
Business Logic: Did you check that the data makes sense given your knowledge of the business?

Review the goal of your project
Once you have finished these data cleaning tasks, it is a good idea to review the goal of your project and confirm that your data is still aligned with that goal. This is a continuous process that you will do throughout your project-- but here are three steps you can keep in mind while thinking about this:

Confirm the business problem
Confirm the goal of the project
Verify that data can solve the problem and is aligned to the goal


Documentation which is the process of tracking changes, additions, deletions and errors involved in your data cleaning effort.

Having a record of how a data set evolved does three very important things.

First, it lets us recover data-cleaning errors.
Second, documentation gives you a way to inform other users of changes you've made
Third, documentation helps you to determine the quality of the data to be used in analysis


As a reminder, a changelog is a file containing a chronologically ordered list of modifications made to a project. You can use and view a changelog in spreadsheets and SQL to achieve similar results

We can use Sheet's version history, which provides a real-time tracker of all the changes and who made them from individual cells to the entire worksheet. To find this feature, click the File tab, and then select Version history.
The way you create and view a changelog with SQL depends on the software program you're using. Some companies even have their own separate software that keeps track of changelogs and important SQL queries. This gets pretty advanced. Essentially, all you have to do is specify exactly what you did and why when you commit a query to the repository as a new and improved query.
I'm in the Query history tab. Listed on the bottom right are all the queries that run by date and time. You can click on this icon to the right of each individual query to bring it up to the Query editor.

Engineers use engineering change orders (ECOs) to keep track of new product design details and proposed changes to existing products. Writers use document revision histories to keep track of changes to document flow and edits. And data analysts use changelogs to keep track of data transformation and cleaning.

Google Sheets
1. Right-click the cell and select Show edit history.
2. Click the left-arrow < or right arrow > to move backward and forward in the history as needed.

Microsoft Excel
1. If Track Changes has been enabled for the spreadsheet: click Review.
2. Under Track Changes, click the Accept/Reject Changes option to accept or reject any change made.

BigQuery
Bring up a previous version (without reverting to it) and figure out what changed by comparing it to the current version.

If an analyst is making changes to an existing SQL query that is shared across the company, the company most likely uses what is called a version control system.


Here is how a version control system affects a change to a query:

A company has official versions of important queries in their version control system.
An analyst makes sure the most up-to-date version of the query is the one they will change. This is called syncing
The analyst makes a change to the query.
The analyst might ask someone to review this change. This is called a code review and can be informally or formally done. An informal review could be as simple as asking a senior analyst to take a look at the change.
After a reviewer approves the change, the analyst submits the updated version of the query to a repository in the company's version control system. This is called a code commit. A best practice is to document exactly what the change was and why it was made in a comments area. Going back to our example of a query that pulls daily revenue, a comment might be: Updated revenue to include revenue coming from the new product, Calypso.
After the change is submitted, everyone else in the company will be able to access and use this new query when they sync to the most up-to-date queries stored in the version control system.
If the query has a problem or business needs change, the analyst can undo the change to the query using the version control system. The analyst can look at a chronological list of all changes made to the query and who made each change. Then, after finding their own change, the analyst can revert to the previous version.
The query is back to what it was before the analyst made the change. And everyone at the company sees this reverted, original query, too.

In this reading, you will learn about some advanced functions that can help you speed up the data cleaning process in spreadsheets. Below is a table summarizing three functions and what they do:

Function:
IMPORTRANGE
QUERY
FILTER

Syntax:
Google Sheets:
=IMPORTRANGE(spreadsheet url, range_string)
=QUERY(Sheet and Range, "Select *")
=FILTER(range, condition1, conditionN.....)

Excel:
Paste Link (copy data first)
Data > From Other Sources > From Microsoft Query
Filter (Conditions per Column)

Primary Use:
Allows you to insert data from one sheet to another. Automatically updated.
Enables SQL statements or wizard for importing data
Enables only the data that meets a specified condition




Week 5

Now, there's more than one way to build a resume, but most have contact information at the top of the document. This includes your name, address, phone number, and email address. If you have multiple email addresses or phone numbers, use the ones that are most reliable and sound professional.

A format that focuses more on skills and qualifications and less on work history is great for people who have gaps in their work history. It's also good for those who are just starting out their career or making a career change, and that might be you.

Once you've completed this program and have your certificate, you'll be able to include that too, which could sound like this, "entry-level data analytics professional recently completed the Google Data Analytics Professional Certificate." Sounds pretty good, doesn't it?
If you're including a work experience section, there's lots of different types of experience you could add. Outside of jobs with other companies, you could also include volunteer positions you've had and any freelance or side work you've done. The key here is the way in which you describe these experiences. Try to describe the work you did in a way that relates to the position you're applying for. Most job descriptions have minimum qualifications or requirements listed. These are the experiences, skills, and education you'll need to be considered for the job. It's important to clearly state them in your resume.

If a job listing describes a job responsibility as "effectively managing data resources," you'll want to have your own description that reflects that responsibility. For example, if you volunteered or worked at a local school or community center, you might say that you "effectively managed resources for after-school activities." Later on, you'll learn more ways to make your work history work for you. It's helpful to describe your skills and qualifications in the same way.

One way to do this is to follow a formula in your descriptions: Accomplished X as measured by Y, by doing Z. Here's an example of how this might read on a resume: Selected as one of 275 participants nationwide for this 12-month professional development program for high- achieving talent based on leadership potential and academic success.

If you've gained new skills in one of your experiences, be sure to highlight them all and how they helped. This is probably as good a spot as any to bring up data analytics. Even if this program is the first time you really thought about data analytics, now that you're equipped with some knowledge, you'll want to use that to your benefit.

Effectively implemented and communicated daily workflow to fellow team members, resulting in an increase in productivity.



Adding professional skills to your resume

1. Structured Query Language (SQL): SQL is considered a basic skill that is pivotal to any entry-level data analyst position. SQL helps you communicate with databases, and more specifically, it is designed to help you retrieve information from databases. Every month, thousands of data analyst jobs posted require SQL, and knowing how to use SQL remains one of the most common job functions of a data analyst.

2. Spreadsheets: Although SQL is popular, 62% of companies still prefer to use spreadsheets for their data insights. When getting your first job as a data analyst, the first version of your database might be in spreadsheet form, which is still a powerful tool for reporting or even presenting data sets. So, it is important for you to be familiar with using spreadsheets for your data insights.

3. Data visualization tools: Data visualization tools help to simplify complex data and enable the data to be visually understood. After gathering and analyzing data, data analysts are tasked with presenting their findings and making that information simple to grasp. Common tools that are used in data analysis include Tableau, Microstrategy, Data Studio, Looker, Datarama, Microsoft Power BI, and many more. Among these, Tableau is best known for its ease of use, so it is a must-have for beginner data analysts. Also, studies show that data analysis jobs requiring Tableau are expected to grow about 34.9% over the next decade.

4. R or Python programming: Since only less than a third of entry-level data analyst positions require knowledge of Python or R, you don’t need to be proficient in programming languages as an entry-level data analyst. But, R or Python are great additions to have as you become more advanced in your career.




Adding soft skills to your resume

1. Presentation skills

Although gathering and analyzing data is a big part of the job, presenting your findings in a clear and simple way is just as important. You will want to structure your findings in a way that allows your audience to know exactly what conclusions they are supposed to draw.

2. Collaboration

As a data analyst, you will be asked to work with lots of teams and stakeholders—sometimes internal or external—and your ability to share ideas, insights, and criticisms will be crucial. It is important that you and your team—which might consist of engineers and researchers—do your best to get the job done.

3. Communication

Data analysts must communicate effectively to obtain the data that they need. It is also important that you are able to work and clearly communicate with teams and business leaders in a language that they understand.

4. Research

As a data analyst, even if you have all of the data at your disposal, you still need to analyze it and draw crucial insights from it. To analyze the data and draw conclusions, you will need to conduct research to stay in-line with industry trends.

5. Problem-solving skills

Problem-solving is a big part of a data analyst’s job, and you will encounter times when there are errors in databases, code, or even the capturing of data. You will have to adapt and think outside the box to find alternative solutions to these problems.

6. Adaptability

In the ever-changing world of data, you have to be adaptable and flexible. As a data analyst, you will be working across multiple teams with different levels of needs and knowledge, which requires you to adjust to different teams, knowledge levels, and stakeholders.

7. Attention to detail

A single line of incorrect code can throw everything off, so paying attention to detail is critical for a data analyst. When it comes to understanding and reporting findings, it helps if you focus on the details that matter to your audience.


Glossary Terms:

A/B testing: The process of testing two variations of the same web page to determine which page is more successful at attracting user traffic and generating revenue
Access control: Features such as password protection, user permissions, and encryption that are used to protect a spreadsheet
Accuracy: The degree to which data conforms to the actual entity being measured or described
Action-oriented question: A question whose answers lead to change
Administrative metadata: Metadata that indicates the technical source of a digital asset
Agenda: A list of scheduled appointments
Algorithm: A process or set of rules followed for a specific task
Analytical skills: Qualities and characteristics associated with using facts to solve problems
Analytical thinking: The process of identifying and defining a problem, then solving it by using data in an organized, step-by-step manner
Attribute: A characteristic or quality of data used to label a column in a table
Audio file: Digitized audio storage usually in an MP3, AAC, or other compressed format
AVERAGE: A spreadsheet function that returns an average of the values from a selected range
Bad data source: A data source that is not reliable, original, comprehensive, current, and cited (ROCCC)
Bias: A conscious or subconscious preference in favor of or against a person, group of people, or thing
Big data: Large, complex datasets typically involving long periods of time, which enable data analysts to address far-reaching business problems
Boolean data: A data type with only two possible values, usually true or false
Borders: Lines that can be added around two or more cells on a spreadsheet
Business task: The question or problem data analysis answers for a business
CASE: A SQL statement that returns records that meet conditions by including an if/then statement in a query
CAST: A SQL function that converts data from one datatype to another
Cell reference: A cell or a range of cells in a worksheet typically used in formulas and functions
Changelog: A file containing a chronologically ordered list of modifications made to a project
Clean data: Data that is complete, correct, and relevant to the problem being solved
Cloud: A place to keep data online, rather than a computer hard drive
COALESCE: A SQL function that returns non-null values in a list
Compatibility: How well two or more datasets are able to work together
Completeness: The degree to which data contains all desired components or measures
CONCAT: A SQL function that adds strings together to create new text strings that can be used as unique keys
CONCATENATE: A spreadsheet function that joins together two or more text strings
Conditional formatting: A spreadsheet tool that changes how cells appear when values meet specific conditions
Confidence interval:  A range of values that conveys how likely a statistical estimate reflects the population
Confidence level: The probability that a sample size accurately reflects the greater population
Confirmation bias: The tendency to search for or interpret information in a way that confirms pre-existing beliefs
Consent: The aspect of data ethics that presumes an individual’s right to know how and why their personal data will be used before agreeing to provide it
Consistency: The degree to which data is repeatable from different points of entry or collection
Context: The condition in which something exists or happens
Continuous data: Data that is measured and can have almost any numeric value
Cookie: A small file stored on a computer that contains information about its users
COUNT: A spreadsheet function that counts the number of cells in a range that meet a specified criteria
COUNTA: A spreadsheet function that counts the total number of values within a specified range
COUNTIF: A spreadsheet function that returns the number of cells in a range that match a specified value
Cross-field validation: A process that ensures certain conditions for multiple data fields are satisfied
CSV (comma-separated values) file: A delimited text file that uses a comma to separate values
Currency: The aspect of data ethics that presumes individuals should be aware of financial transactions resulting from the use of their personal data and the scale of those transactions
Dashboard: A tool that monitors live, incoming data
Data: A collection of facts
Data analysis: The collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making
Data analysis process: The six phases of ask, prepare, process, analyze, share, and act whose purpose is to gain insights that drive informed decision-making
Data analyst: Someone who collects, transforms, and organizes data in order to draw conclusions, make predictions, and drive informed decision-making
Data analytics: The science of data
Data anonymization: The process of protecting people's private or sensitive data by eliminating identifying information
Data bias: When a preference in favor of or against a person, group of people, or thing systematically skews data analysis results in a certain direction
Data constraints: The criteria that determine whether a piece of a data is clean and valid
Data design: How information is organized
Data-driven decision-making: Using facts to guide business strategy
Data ecosystem: The various elements that interact with one another in order to produce, manage, store, organize, analyze, and share data
Data element: A piece of information in a dataset
Data engineer: A professional who transforms data into a useful format for analysis and gives it a reliable infrastructure
Data ethics: Well-founded standards of right and wrong that dictate how data is collected, shared, and used
Data governance: A process for ensuring the formal management of a company’s data assets
Data-inspired decision-making: Exploring different data sources to find out what they have in common
Data integrity: The accuracy, completeness, consistency, and trustworthiness of data throughout its life cycle
Data interoperability: The ability to integrate data from multiple sources and a key factor leading to the successful use of open data among companies and governments
Data life cycle: The sequence of stages that data experiences, which include plan, capture, manage, analyze, archive, and destroy
Data manipulation: The process of changing data to make it more organized and easier to read
Data mapping: The process of matching fields from one data source to another
Data merging: The process of combining two or more datasets into a single dataset
Data model: A tool for organizing data elements and how they relate to one another
Data privacy: Preserving a data subject’s information any time a data transaction occurs
Data range: Numerical values that fall between predefined maximum and minimum values
Data replication: The process of storing data in multiple locations
Data science: A field of study that uses raw data to create new ways of modeling and understanding the unknown
Data security: Protecting data from unauthorized access or corruption by adopting safety measures
Data strategy: The management of the people, processes, and tools used in data analysis
Data transfer: The process of copying data from a storage device to computer memory or from one computer to another
Data type: An attribute that describes a piece of data based on its values, its programming language, or the operations it can perform
Data validation: A tool for checking the accuracy and quality of data
Data visualization: The graphical representation of data
Data warehousing specialist: A professional who develops processes and procedures to effectively store and organize data
Database: A collection of data stored in a computer system
Dataset: A collection of data that can be manipulated or analyzed as one unit
DATEDIF: A spreadsheet function that calculates the number of days, months, or years between two dates
Delimiter: A character that indicates the beginning or end of a data item
Descriptive metadata: Metadata that describes a piece of data and can be used to identify it at a later point in time
Digital photo: An electronic or computer-based image usually in BMP or JPG format
Dirty data: Data that is incomplete, incorrect, or irrelevant to the problem to be solved
Discrete data: Data that is counted and has a limited number of values
DISTINCT: A keyword that is added to a SQL SELECT statement to retrieve only non-duplicate entries
Duplicate data: Any record that inadvertently shares data with another record
Equation: A calculation that involves addition, subtraction, multiplication, or division (also called a math expression)
Estimated response rate: The average number of people who typically complete a survey
Ethics: Well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness, or specific virtues
Experimenter bias: The tendency for different people to observe things differently (Refer to Observer bias)
External data: Data that lives, and is generated, outside of an organization
Fairness: A quality of data analysis that does not create or reinforce bias
Field: A single piece of information from a row or column of a spreadsheet; in a data table, typically a column in the table
Field length: A tool for determining how many characters can be keyed into a spreadsheet field
Fill handle: A box in the lower-right-hand corner of a selected spreadsheet cell that can be dragged through neighboring cells in order to continue an instruction
Filtering: The process of showing only the data that meets a specified criteria while hiding the rest
Find and replace: A tool that finds a specified search term and replaces it with something else
First-party data: Data collected by an individual or group using their own resources
Float: A number that contains a decimal
Foreign key: A field within a database table that is a primary key in another table (Refer to primary key)
Formula: A set of instructions used to perform a calculation using the data in a spreadsheet
FROM: The section of a query that indicates from which table(s) to extract the data
Function: A preset command that automatically performs a specified process or task using the data in a spreadsheet
Gap analysis: A method for examining and evaluating the current state of a process in order to identify opportunities for improvement in the future
General Data Protection Regulation of the European Union (GDPR): Policy-making body in the European Union created to help protect people and their data
Geolocation: The geographical location of a person or device by means of digital information
Good data source: A data source that is reliable, original, comprehensive, current, and cited (ROCCC)
Header: The first row in a spreadsheet that labels the type of data in each column
Hypothesis testing: A process to determine if a survey or experiment has meaningful results
Incomplete data: Data that is missing important fields
Inconsistent data: Data  that uses different formats to represent the same thing
Incorrect/inaccurate data: Data  that is complete but inaccurate
Internal data: Data that lives within a company’s own systems
Interpretation bias: The tendency to interpret ambiguous situations in a positive or negative way
Leading question: A question that steers people toward a certain response
LEFT: A function that returns a set number of characters from the left side of a text string
LEN: A function that returns the length of a text string by counting the number of characters it contains
Length: The number of characters in a text string
Long data: A dataset in which each row is one time point per subject, so each subject has data in multiple rows
Mandatory: A data value that cannot be left blank or empty
Margin of error: The maximum amount that sample results are expected to differ from those of the actual population
Math expression: A calculation that involves addition, subtraction, multiplication, or division (also called an equation)
Math function: A function that is used as part of a mathematical formula
MAX: A function that returns the largest numeric value from a range of cells
Measurable question: A question whose answers can be quantified and assessed
Mentor: Someone who shares knowledge, skills, and experience to help another grow both professionally and personally
Merger: An agreement that unites two organizations into a single new one
Metadata: Data about data
Metadata repository: A database created to store metadata
Metric: A single, quantifiable type of data that is used for measurement
Metric goal: A measurable goal set by a company and evaluated using metrics
MID: A function that returns a segment from the middle of a text string
MIN: A spreadsheet function that returns the smallest numeric value from a range of cells
Naming conventions: Consistent guidelines that describe the content, creation date, and version of a file in its name
Networking: Building relationships by meeting people both in person and online
Nominal data: A type of qualitative data that is categorized without a set order
Normalized database: A database in which only related data is stored in each table
Notebook: An interactive, editable programming environment for creating data reports and showcasing data skills
Null: An indication that a value does not exist in a dataset
Observation: The attributes that describe a piece of data contained in a row of a table
Observer bias: The tendency for different people to observe things differently (also called experimenter bias)
Open data: Data that is available to the public
Openness: The aspect of data ethics that promotes the free access, usage, and sharing of data
Operator: A symbol that names the operation or calculation to be performed
Order of operations: Using parentheses to group together spreadsheet values in order to clarify the order in which operations should be performed
Ordinal data: Qualitative data with a set order or scale
Outdated data: Any data that has been superseded by newer and more accurate information
Ownership: The aspect of data ethics that presumes individuals own the raw data they provide and have primary control over its usage, processing, and sharing
Pivot chart: A chart created from the fields in a pivot table
Pivot table: A data summarization tool used to sort, reorganize, group, count, total, or average data
Pixel: In digital imaging, a small area of illumination on a display screen that, when combined with other adjacent areas, forms a digital image
Population: In data analytics, all possible data values in a dataset
Primary key: An identifier in a database that references a column in which each value is unique (Refer to foreign key)
Problem domain: The area of analysis that encompasses every activity affecting or affected by a problem
Problem types: The various problems that data analysts encounter, including categorizing things, discovering connections, finding patterns, identifying themes, making predictions, and spotting something unusual
Qualitative data: A subjective and explanatory measure of a quality or characteristic
Quantitative data: A specific and objective measure, such as a number, quantity, or range
Query: A request for data or information from a database
Query language: A computer programming language used to communicate with a database
Random sampling: A way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen
Range: A collection of two or more cells in a spreadsheet
Record: A collection of related data in a data table, usually synonymous with row
Redundancy: When the same piece of data is stored in two or more places
Reframing: The process of restating a problem or challenge, then redirecting it toward a potential resolution
Regular expression (RegEx): A rule that says the values in a table must match a prescribed pattern
Relational database: A database that contains a series of tables that can be connected to form relationships
Relevant question: A question that has significance to the problem to be solved
Remove duplicates: A spreadsheet tool that automatically searches for and eliminates duplicate entries from a spreadsheet
Report: A static collection of data periodically given to stakeholders
Return on investment (ROI): A formula that uses the metrics of investment and profit to evaluate the success of an investment
Revenue: The total amount of income generated by the sale of goods or services
RIGHT: A function that returns a set number of characters from the right side of a text string
Root cause: The reason why a problem occurs
Sample: In data analytics, a segment of a population that is representative of the entire population
Sampling bias: Overrepresenting or underrepresenting certain members of a population as a result of working with a sample that is not representative of the population as a whole
Schema: A way of describing how something, such as data, is organized
Scope of work (SOW): An agreed-upon outline of the tasks to be performed during a project
Second-party data: Data collected by a group directly from its audience and then sold
SELECT: The section of a query that indicates from which column(s) to extract the data
Small data: Small, specific data points typically involving a short period of time, which are useful for making day-to-day decisions
SMART methodology: A tool for determining a question’s effectiveness based on whether it is specific, measurable, action-oriented, relevant, and time-bound
Social media: Websites and applications through which users create and share content or participate in social networking
Soft skills: Nontechnical traits and behaviors that relate to how people work
Sorting: The process of arranging data into a meaningful order to make it easier to understand, analyze, and visualize
Specific question: A question that is simple, significant, and focused on a single topic or a few closely related ideas
Split: A spreadsheet function that divides text around a specified character and puts each fragment into a new, separate cell
Sponsor: A professional advocate who is committed to moving forward the career of another
Spreadsheet: A digital worksheet
SQL: (Refer to Structured Query Language)
Stakeholders: People who invest time and resources into a project and are interested in its outcome
Statistical power: The probability that a test of significance will recognize an effect that is present
Statistical significance: The probability that sample results are not due to random chance
String data type: A sequence of characters and punctuation that contains textual information (also called text data type)
Structural metadata: Metadata that indicates how a piece of data is organized and whether it is part of one or more than one data collection
Structured data: Data organized in a certain format such as rows and columns
Structured Query Language: A computer programming language used to communicate with a database
Structured thinking: The process of recognizing the current problem or situation, organizing available information, revealing gaps and opportunities, and identifying options
SUBSTR: A SQL function that extracts a substring from a string variable
Substring: A subset of a text string
SUM: A spreadsheet function that adds the values of a selected range of cells
Syntax: The predetermined structure of a language that includes all required words, symbols, and punctuation, as well as their proper placement
Technical mindset: The ability to break things down into smaller steps or pieces and work with them in an orderly and logical way
Text data type: A sequence of characters and punctuation that contains textual information (also called string data type)
Text string: A group of characters within a cell, most often composed of letters
Third-party data: Data provided from outside sources who didn’t collect it directly
Time-bound question: A question that specifies a timeframe to be studied
Transaction transparency: The aspect of data ethics that presumes all data-processing activities and algorithms should be explainable and understood by the individual who provides the data
Transferable skills: Skills and qualities that can transfer from one job or industry to another
TRIM: A function that removes leading, trailing, and repeated spaces in data
Turnover rate: The rate at which employees voluntarily leave a company
Typecasting: Converting data from one type to another
Unbiased sampling: When the sample of the population being measured is representative of the population as a whole
Unfair question: A question that makes assumptions or is difficult to answer honestly
Unique: A value that can’t have a duplicate
United States Census Bureau: An agency in the U.S. Department of Commerce that serves as the nation’s leading provider of quality data about its people and economy
Unstructured data: Data that is not organized in any easily identifiable manner
Validity: The degree to which data conforms to constraints when it is input, collected, or created
Verification: A process to confirm that a data-cleaning effort was well executed and the resulting data is accurate and reliable
Video file: A collection of images, audio files, and other data usually encoded in a compressed format such as MP4, MV4, MOV, AVI, or FLV
Visualization: (Refer to Data visualization)
VLOOKUP: A spreadsheet function that vertically searches for a certain value in a column to return a corresponding piece of information
WHERE: The section of a query that specifies criteria that the requested data must meet
Wide data: A dataset in which every data subject has a single row with multiple columns to hold the values of various attributes of the subject
World Health Organization: An organization whose primary role is to direct and coordinate international health within the United Nations system

